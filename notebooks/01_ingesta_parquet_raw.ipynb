{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRE1nFzk1e2u"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_Dz_d9a4Rj3"
      },
      "source": [
        "## Descarga de .parquet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdIi5dze30RU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Descarga datos yellow y green + taxi_zone_lookup para todos los meses de 2015 a 2025\n",
        "start_year = 2015\n",
        "end_year = 2025\n",
        "months = range(1, 13)\n",
        "base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
        "zone_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
        "data_dir = '/home/jovyan/work/data'  # Carpeta montada por Docker, accesible desde local\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "missing_files = []\n",
        "for year in range(start_year, end_year + 1):\n",
        "    for color in ['yellow', 'green']:\n",
        "        for m in months:\n",
        "            fname = f'{color}_tripdata_{year}-{m:02d}.parquet'\n",
        "            url = f'{base_url}/{fname}'\n",
        "            dest = f'{data_dir}/{fname}'\n",
        "            exit_code = os.system(f'wget -O {dest} {url}')\n",
        "            if exit_code != 0 or not Path(dest).is_file():\n",
        "                missing_files.append(fname)\n",
        "# Descarga taxi_zone_lookup\n",
        "zone_dest = f'{data_dir}/taxi_zone_lookup.csv'\n",
        "exit_code = os.system(f'wget -O {zone_dest} {zone_url}')\n",
        "if exit_code != 0 or not Path(zone_dest).is_file():\n",
        "    missing_files.append('taxi_zone_lookup.csv')\n",
        "\n",
        "# Resumen\n",
        "if missing_files:\n",
        "    print('Faltan los siguientes archivos:')\n",
        "    for f in missing_files:\n",
        "        print('-', f)\n",
        "else:\n",
        "    print('Todos los archivos descargados correctamente.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFi1zkH4Wrm"
      },
      "source": [
        "\n",
        "## instalación de jar snowflake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xLONPyQ339C"
      },
      "outputs": [],
      "source": [
        "# Instalación de dependencias y descarga de JARs para Spark-Snowflake\n",
        "!pip install snowflake-snowpark-python snowflake-connector-python\n",
        "\n",
        "# Crear directorio para JARs si no existe\n",
        "import os\n",
        "jars_dir = '/home/jovyan/work/jars'\n",
        "os.makedirs(jars_dir, exist_ok=True)\n",
        "\n",
        "# Descargar JARs necesarios para Spark 3.x con Scala 2.12\n",
        "# Snowflake Spark Connector compatible con Spark 3.x y Scala 2.12\n",
        "snowflake_jar_url = \"https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.12.0-spark_3.4/spark-snowflake_2.12-2.12.0-spark_3.4.jar\"\n",
        "snowflake_jdbc_url = \"https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.14.4/snowflake-jdbc-3.14.4.jar\"\n",
        "\n",
        "# Descargar JARs\n",
        "!wget -O {jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar {snowflake_jar_url}\n",
        "!wget -O {jars_dir}/snowflake-jdbc-3.14.4.jar {snowflake_jdbc_url}\n",
        "\n",
        "print(\"JARs descargados exitosamente\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypDBdSo738_u",
        "outputId": "60208ab8-0c1f-4c02-9c1f-16b3c5c833b2"
      },
      "outputs": [],
      "source": [
        "# INICIALIZAR SPARK SESSION\n",
        "print(\"=== INICIALIZANDO SPARK ===\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "# Configuración de JARs para Snowflake\n",
        "jars_dir = '/home/jovyan/work/jars'\n",
        "spark_jars = f\"{jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar,{jars_dir}/snowflake-jdbc-3.14.4.jar\"\n",
        "\n",
        "# Crear sesión Spark simple\n",
        "print(\"Creando sesión Spark...\")\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC_Taxi_Ingesta_Simple\") \\\n",
        "    .config(\"spark.jars\", spark_jars) \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verificar que funciona\n",
        "test_count = spark.range(5).count()\n",
        "print(f\"Spark funcionando correctamente - Test: {test_count} registros\")\n",
        "print(f\"Versión Spark: {spark.version}\")\n",
        "\n",
        "# Mostrar configuración activa\n",
        "print(f\"JARs configurados: {len(spark_jars.split(','))} archivos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCzvPeJUdY_2"
      },
      "source": [
        "## Ingesta taxi zones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXqsqj6CdXY2"
      },
      "outputs": [],
      "source": [
        "# INGESTA TAXI_ZONE_LOOKUP\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Leer variables de entorno\n",
        "SNOWFLAKE_ACCOUNT = os.getenv('SNOWFLAKE_ACCOUNT')\n",
        "SNOWFLAKE_USER = os.getenv('SNOWFLAKE_USER')\n",
        "SNOWFLAKE_PASSWORD = os.getenv('SNOWFLAKE_PASSWORD')\n",
        "SNOWFLAKE_DATABASE = os.getenv('SNOWFLAKE_DATABASE')\n",
        "SNOWFLAKE_SCHEMA_RAW = os.getenv('SNOWFLAKE_SCHEMA_RAW', 'RAW')\n",
        "SNOWFLAKE_WAREHOUSE = os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH')\n",
        "\n",
        "\n",
        "# Configuración Snowflake para taxi zones\n",
        "\n",
        "sfOptions = {\n",
        "    \"sfURL\": SNOWFLAKE_ACCOUNT,\n",
        "    \"sfUser\": SNOWFLAKE_USER,\n",
        "    \"sfPassword\": SNOWFLAKE_PASSWORD,\n",
        "    \"sfDatabase\": SNOWFLAKE_DATABASE,\n",
        "    \"sfSchema\": SNOWFLAKE_SCHEMA_RAW,\n",
        "    \"sfWarehouse\": SNOWFLAKE_WAREHOUSE,\n",
        "    \"timezone\": \"UTC\"\n",
        "}\n",
        "\n",
        "# Configuración para taxi zones\n",
        "data_dir = '/home/jovyan/work/data'\n",
        "zone_file = 'taxi_zone_lookup.csv'\n",
        "zone_path = os.path.join(data_dir, zone_file)\n",
        "run_id = f\"taxi_zones_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "ingested_at_utc = datetime.utcnow().isoformat()\n",
        "\n",
        "print(f\"INICIANDO INGESTA TAXI ZONES\")\n",
        "print(f\"Run ID: {run_id}\")\n",
        "print(f\"Archivo: {zone_file}\")\n",
        "\n",
        "# Verificar que el archivo existe\n",
        "if not os.path.isfile(zone_path):\n",
        "    print(f\"ERROR: Archivo {zone_file} no encontrado en {data_dir}\")\n",
        "else:\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        # PASO 1: Leer archivo CSV\n",
        "        print(\"Leyendo archivo taxi_zone_lookup.csv...\")\n",
        "        df_zones = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(zone_path)\n",
        "\n",
        "        # Mostrar esquema y sample\n",
        "        print(\"Esquema detectado:\")\n",
        "        df_zones.printSchema()\n",
        "\n",
        "        # Cache para evitar re-lecturas\n",
        "        df_zones.cache()\n",
        "        total_zones = df_zones.count()\n",
        "        print(f\"Total zonas encontradas: {total_zones:,}\")\n",
        "\n",
        "        if total_zones == 0:\n",
        "            print(\"ADVERTENCIA: Archivo vacío\")\n",
        "        else:\n",
        "            # Mostrar muestra de datos\n",
        "            print(\"\\nMuestra de datos:\")\n",
        "            df_zones.show(5, truncate=False)\n",
        "\n",
        "            # PASO 2: Transformar datos con casting explícito\n",
        "            print(\"Aplicando transformaciones de tipos...\")\n",
        "\n",
        "            # Solo datos originales con casting para consistencia\n",
        "            df_zones_final = df_zones.select(\n",
        "                F.col('LocationID').cast(T.IntegerType()).alias('LocationID'),\n",
        "                F.col('Borough').cast(T.StringType()).alias('Borough'),\n",
        "                F.col('Zone').cast(T.StringType()).alias('Zone'),\n",
        "                F.col('service_zone').cast(T.StringType()).alias('service_zone')\n",
        "            )\n",
        "\n",
        "            # Cache del DataFrame final\n",
        "            df_zones_final.cache()\n",
        "            df_zones.unpersist()  # Liberar original\n",
        "\n",
        "            # Verificar datos finales\n",
        "            final_count = df_zones_final.count()\n",
        "            print(f\"Registros después de transformación: {final_count:,}\")\n",
        "\n",
        "            # PASO 3: Cargar a Snowflake\n",
        "            table_name = \"TAXI_ZONE_LOOKUP\"\n",
        "            print(f\"Cargando {final_count:,} registros a tabla {table_name}...\")\n",
        "\n",
        "            # Escribir a Snowflake\n",
        "            df_zones_final.write \\\n",
        "                .format(\"net.snowflake.spark.snowflake\") \\\n",
        "                .options(**sfOptions) \\\n",
        "                .option(\"dbtable\", table_name) \\\n",
        "                .mode(\"overwrite\") \\\n",
        "                .save()  # OVERWRITE porque es tabla de referencia\n",
        "\n",
        "            # Limpieza de memoria\n",
        "            df_zones_final.unpersist()\n",
        "\n",
        "            # Métricas finales\n",
        "            processing_time = time.time() - start_time\n",
        "\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"INGESTA TAXI ZONES COMPLETADA\")\n",
        "            print(f\"{'='*50}\")\n",
        "            print(f\"Registros procesados: {final_count:,}\")\n",
        "            print(f\"Tiempo total: {processing_time:.2f} segundos\")\n",
        "            print(f\"Velocidad: {final_count/processing_time:,.0f} registros/segundo\")\n",
        "            print(f\"Tabla: {table_name} (OVERWRITE)\")\n",
        "            print(f\"Estado: ÉXITO\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR en ingesta de taxi zones: {e}\")\n",
        "        # Limpieza en caso de error\n",
        "        for var_name in ['df_zones', 'df_zones_final']:\n",
        "            try:\n",
        "                if var_name in locals():\n",
        "                    locals()[var_name].unpersist()\n",
        "            except:\n",
        "                pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Vmpol7u4d1i"
      },
      "source": [
        "##  Ingesta Green\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjE655wU4ALB",
        "outputId": "d42d8a95-7536-438b-c41b-a9bf0a7ab3ee"
      },
      "outputs": [],
      "source": [
        "# Configuración Snowflake con manejo correcto de timestamps\n",
        "\n",
        "sfOptions = {\n",
        "    \"sfURL\": SNOWFLAKE_ACCOUNT,\n",
        "    \"sfUser\": SNOWFLAKE_USER,\n",
        "    \"sfPassword\": SNOWFLAKE_PASSWORD,\n",
        "    \"sfDatabase\": SNOWFLAKE_DATABASE,\n",
        "    \"sfSchema\": SNOWFLAKE_SCHEMA_RAW,\n",
        "    \"sfWarehouse\": SNOWFLAKE_WAREHOUSE,\n",
        "    \"timezone\": \"UTC\",\n",
        "        # Opciones para manejo correcto de timestamps\n",
        "    \"timezone\": \"UTC\",\n",
        "    \"timestampFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\",\n",
        "    \"timestampLtzFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\",\n",
        "    \"timestampNtzFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\"\n",
        "}\n",
        "\n",
        "# Importar datetime si no está disponible\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuración\n",
        "service_types = ['green']\n",
        "start_year = 2016\n",
        "end_year = 2025\n",
        "months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "data_dir = '/home/jovyan/work/data'\n",
        "\n",
        "run_id = f\"raw_fixed_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "ingested_at_utc = datetime.utcnow().isoformat()\n",
        "\n",
        "print(f\"Run ID: {run_id}\")\n",
        "\n",
        "# Procesamiento simplificado\n",
        "total_files_processed = 0\n",
        "total_rows_ingested = 0\n",
        "audit_records = []\n",
        "\n",
        "for service_type in service_types:\n",
        "    print(f\"Servicio: {service_type.upper()}\")\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in months:\n",
        "            fname = f'{service_type}_tripdata_{year}-{month:02d}.parquet'\n",
        "            fpath = os.path.join(data_dir, fname)\n",
        "\n",
        "            print(f\"Procesando: {fname}\")\n",
        "\n",
        "            if not os.path.isfile(fpath):\n",
        "                print(f\"Archivo no encontrado: {fname}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Leer archivo\n",
        "                df = spark.read.parquet(fpath)\n",
        "                original_count = df.count()\n",
        "                print(f\"Registros originales: {original_count:,}\")\n",
        "\n",
        "                if original_count == 0:\n",
        "                    continue\n",
        "\n",
        "                # Agregar metadatos\n",
        "                pickup_col = 'lpep_pickup_datetime' if service_type == 'green' else 'tpep_pickup_datetime'\n",
        "                natural_key = F.concat_ws('|',\n",
        "                    F.coalesce(F.col(pickup_col).cast('string'), F.lit('NULL')),\n",
        "                    F.coalesce(F.col('VendorID').cast('string'), F.lit('NULL'))\n",
        "                )\n",
        "\n",
        "                # Crear DataFrame con metadatos AL INICIO + todas las columnas originales\n",
        "                df = df.select(\n",
        "                    # METADATOS PRIMERO (posiciones 1-7 como en Snowflake)\n",
        "                    F.lit(run_id).cast(T.StringType()).alias('run_id'),\n",
        "                    F.lit(service_type).cast(T.StringType()).alias('service_type'),\n",
        "                    F.lit(year).cast(T.IntegerType()).alias('source_year'),\n",
        "                    F.lit(month).cast(T.IntegerType()).alias('source_month'),\n",
        "                    F.lit(ingested_at_utc).cast(T.StringType()).alias('ingested_at_utc'),\n",
        "                    F.lit(fpath).cast(T.StringType()).alias('source_path'),\n",
        "                    natural_key.alias('natural_key'),\n",
        "                    # *** INCLUIR TODAS LAS COLUMNAS ORIGINALES ***\n",
        "                    *[F.col(c) for c in df.columns]\n",
        "                )\n",
        "\n",
        "                # PASO 6: CONVERTIR TIMESTAMPS PARA SNOWFLAKE\n",
        "                print(\"Convirtiendo timestamps...\")\n",
        "\n",
        "                # Convertir SOLO las columnas de pickup/dropoff a TIMESTAMP (compatible con Snowflake)\n",
        "                pickup_col = 'lpep_pickup_datetime' if service_type == 'green' else 'tpep_pickup_datetime'\n",
        "                dropoff_col = 'lpep_dropoff_datetime' if service_type == 'green' else 'tpep_dropoff_datetime'\n",
        "\n",
        "                # Usar TimestampType regular (compatible con conector Snowflake)\n",
        "                if pickup_col in df.columns:\n",
        "                    df = df.withColumn(pickup_col, F.col(pickup_col).cast(T.TimestampType()))\n",
        "\n",
        "                if dropoff_col in df.columns:\n",
        "                    df = df.withColumn(dropoff_col, F.col(dropoff_col).cast(T.TimestampType()))\n",
        "\n",
        "                count_after = df.count()\n",
        "\n",
        "                # PASO 7: SINCRONIZAR ESQUEMA CON TABLA SNOWFLAKE\n",
        "                print(\"Sincronizando esquema...\")\n",
        "\n",
        "                # Definir esquema completo de la tabla Snowflake (sin metadatos)\n",
        "                snowflake_schema = {\n",
        "                    'VendorID': T.IntegerType(),\n",
        "                    'lpep_pickup_datetime': T.TimestampType(),    # TimestampType para compatibilidad\n",
        "                    'lpep_dropoff_datetime': T.TimestampType(),   # TimestampType para compatibilidad\n",
        "                    'store_and_fwd_flag': T.StringType(),\n",
        "                    'RatecodeID': T.IntegerType(),\n",
        "                    'PULocationID': T.IntegerType(),\n",
        "                    'DOLocationID': T.IntegerType(),\n",
        "                    'passenger_count': T.IntegerType(),\n",
        "                    'trip_distance': T.FloatType(),\n",
        "                    'fare_amount': T.FloatType(),\n",
        "                    'extra': T.FloatType(),\n",
        "                    'mta_tax': T.FloatType(),\n",
        "                    'tip_amount': T.FloatType(),\n",
        "                    'tolls_amount': T.FloatType(),\n",
        "                    'ehail_fee': T.IntegerType(),\n",
        "                    'improvement_surcharge': T.FloatType(),\n",
        "                    'total_amount': T.FloatType(),\n",
        "                    'payment_type': T.IntegerType(),\n",
        "                    'trip_type': T.FloatType(),\n",
        "                    'congestion_surcharge': T.FloatType(),\n",
        "                    'airport_fee': T.FloatType(),\n",
        "                    'cbd_congestion_fee': T.FloatType()\n",
        "                }\n",
        "\n",
        "                current_columns = set(df.columns)\n",
        "                target_columns = set(snowflake_schema.keys())\n",
        "                metadata_columns = {'run_id', 'service_type', 'source_year', 'source_month', 'ingested_at_utc', 'source_path', 'natural_key'}\n",
        "\n",
        "                # Agregar columnas que faltan en DataFrame pero están en Snowflake\n",
        "                missing_in_df = target_columns - current_columns\n",
        "                for col_name in sorted(missing_in_df):\n",
        "                    df = df.withColumn(col_name, F.lit(None).cast(snowflake_schema[col_name]))\n",
        "\n",
        "                print(f\"Columnas totales: {len(df.columns)} - Registros: {count_after:,}\")\n",
        "\n",
        "                # PASO 8: Cargar a Snowflake\n",
        "                table_name = \"GREEN_TAXI\" if service_type == 'green' else \"YELLOW_TAXI\"\n",
        "                print(f\"Cargando {count_after:,} registros a {table_name}\")\n",
        "\n",
        "                df.write \\\n",
        "                    .format(\"net.snowflake.spark.snowflake\") \\\n",
        "                    .options(**sfOptions) \\\n",
        "                    .option(\"dbtable\", table_name) \\\n",
        "                    .mode(\"append\") \\\n",
        "                    .save()\n",
        "\n",
        "                print(f\"EXITO: {count_after:,} registros cargados a {table_name}\")\n",
        "\n",
        "                total_files_processed += 1\n",
        "                total_rows_ingested += count_after\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR procesando {fname}: {e}\")\n",
        "                continue\n",
        "\n",
        "print(f\"PROCESO COMPLETADO\")\n",
        "print(f\"Archivos procesados: {total_files_processed}\")\n",
        "print(f\"Total registros: {total_rows_ingested:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agk1wuX04l90"
      },
      "source": [
        "##  Ingesta Yellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPQ8VWVn4GZa",
        "outputId": "1bffa5cf-a112-43dc-8d0a-0e23ea84bb7d"
      },
      "outputs": [],
      "source": [
        "# INGESTA YELLOW TAXI - OPTIMIZADA\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "# Configuración Snowflake optimizada\n",
        "sfOptions = {\n",
        "    \"sfURL\": SNOWFLAKE_ACCOUNT,\n",
        "    \"sfUser\": SNOWFLAKE_USER,\n",
        "    \"sfPassword\": SNOWFLAKE_PASSWORD,\n",
        "    \"sfDatabase\": SNOWFLAKE_DATABASE,\n",
        "    \"sfSchema\": SNOWFLAKE_SCHEMA_RAW,\n",
        "    \"sfWarehouse\": SNOWFLAKE_WAREHOUSE,\n",
        "    \"timezone\": \"UTC\",\n",
        "    \"timestampFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\",\n",
        "    \"timestampLtzFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\",\n",
        "    \"timestampNtzFormat\": \"YYYY-MM-DD HH24:MI:SS.FF\"\n",
        "}\n",
        "\n",
        "def normalize_yellow_columns(df):\n",
        "    \"\"\"Normaliza nombres de columnas para taxi YELLOW\"\"\"\n",
        "    column_mapping = {\n",
        "        'Airport_fee': 'airport_fee',\n",
        "        'AIRPORT_FEE': 'airport_fee',\n",
        "    }\n",
        "\n",
        "    for old_name, new_name in column_mapping.items():\n",
        "        if old_name in df.columns:\n",
        "            df = df.withColumnRenamed(old_name, new_name)\n",
        "            print(f\"Renombrado: {old_name} -> {new_name}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# CONFIGURACIÓN PRINCIPAL\n",
        "service_types = ['yellow']\n",
        "start_year = 2022\n",
        "end_year = 2025\n",
        "months = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
        "#data_dir = '/home/jovyan/work/data'\n",
        "data_dir = '/content/drive/MyDrive/data'\n",
        "\n",
        "run_id = f\"raw_yellow_optimized_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "ingested_at_utc = datetime.utcnow().isoformat()\n",
        "\n",
        "print(f\"INICIANDO INGESTA OPTIMIZADA\")\n",
        "print(f\"Run ID: {run_id}\")\n",
        "print(f\"Configuración: archivos {start_year}-{end_year}, meses {months}\")\n",
        "\n",
        "# Schema ordenado según Snowflake\n",
        "snowflake_columns_ordered = [\n",
        "    'run_id', 'service_type', 'source_year', 'source_month',\n",
        "    'ingested_at_utc', 'source_path', 'natural_key',\n",
        "    'VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
        "    'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag',\n",
        "    'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount',\n",
        "    'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
        "    'total_amount', 'congestion_surcharge', 'airport_fee', 'cbd_congestion_fee'\n",
        "]\n",
        "\n",
        "# Contadores globales\n",
        "total_files_processed = 0\n",
        "total_rows_ingested = 0\n",
        "total_processing_time = 0\n",
        "\n",
        "# PROCESAMIENTO PRINCIPAL OPTIMIZADO\n",
        "for service_type in service_types:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"PROCESANDO {service_type.upper()} TAXI - MODO DIRECTO\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in months:\n",
        "            fname = f'{service_type}_tripdata_{year}-{month:02d}.parquet'\n",
        "            fpath = os.path.join(data_dir, fname)\n",
        "\n",
        "            print(f\"\\nArchivo: {fname}\")\n",
        "\n",
        "            if not os.path.isfile(fpath):\n",
        "                print(f\"Archivo no encontrado: {fname}\")\n",
        "                continue\n",
        "\n",
        "            file_start_time = time.time()\n",
        "\n",
        "            try:\n",
        "                # PASO 1: Lectura del archivo\n",
        "                print(\"Leyendo archivo parquet...\")\n",
        "                df = spark.read.option(\"mergeSchema\", \"true\").parquet(fpath)\n",
        "\n",
        "                # Verificar si está vacío\n",
        "                if df.rdd.isEmpty():\n",
        "                    print(\"Archivo vacío, saltando...\")\n",
        "                    continue\n",
        "\n",
        "                # PASO 2: Normalización de columnas\n",
        "                df = normalize_yellow_columns(df)\n",
        "\n",
        "                # PASO 3: TRANSFORMACIONES CONSOLIDADAS\n",
        "                print(\"Aplicando transformaciones...\")\n",
        "\n",
        "                # Natural key optimizado\n",
        "                natural_key = F.concat_ws('|',\n",
        "                    F.coalesce(F.col('tpep_pickup_datetime').cast('string'), F.lit('NULL')),\n",
        "                    F.coalesce(F.col('VendorID').cast('string'), F.lit('NULL'))\n",
        "                )\n",
        "\n",
        "                # TRANSFORMACIÓN EN UNA SOLA PASADA\n",
        "                df_transformed = df.select(\n",
        "                    # Metadatos\n",
        "                    F.lit(run_id).cast(T.StringType()).alias('run_id'),\n",
        "                    F.lit(service_type).cast(T.StringType()).alias('service_type'),\n",
        "                    F.lit(year).cast(T.IntegerType()).alias('source_year'),\n",
        "                    F.lit(month).cast(T.IntegerType()).alias('source_month'),\n",
        "                    F.lit(ingested_at_utc).cast(T.StringType()).alias('ingested_at_utc'),\n",
        "                    F.lit(fpath).cast(T.StringType()).alias('source_path'),\n",
        "                    natural_key.alias('natural_key'),\n",
        "                    # Datos con casting directo y manejo de nulos\n",
        "                    F.coalesce(F.col('VendorID'), F.lit(None)).cast(T.IntegerType()).alias('VendorID'),\n",
        "                    F.coalesce(F.col('tpep_pickup_datetime'), F.lit(None)).cast(T.TimestampType()).alias('tpep_pickup_datetime'),\n",
        "                    F.coalesce(F.col('tpep_dropoff_datetime'), F.lit(None)).cast(T.TimestampType()).alias('tpep_dropoff_datetime'),\n",
        "                    F.coalesce(F.col('passenger_count'), F.lit(None)).cast(T.IntegerType()).alias('passenger_count'),\n",
        "                    F.coalesce(F.col('trip_distance'), F.lit(None)).cast(T.FloatType()).alias('trip_distance'),\n",
        "                    F.coalesce(F.col('RatecodeID'), F.lit(None)).cast(T.IntegerType()).alias('RatecodeID'),\n",
        "                    F.coalesce(F.col('store_and_fwd_flag'), F.lit(None)).cast(T.StringType()).alias('store_and_fwd_flag'),\n",
        "                    F.coalesce(F.col('PULocationID'), F.lit(None)).cast(T.IntegerType()).alias('PULocationID'),\n",
        "                    F.coalesce(F.col('DOLocationID'), F.lit(None)).cast(T.IntegerType()).alias('DOLocationID'),\n",
        "                    F.coalesce(F.col('payment_type'), F.lit(None)).cast(T.IntegerType()).alias('payment_type'),\n",
        "                    F.coalesce(F.col('fare_amount'), F.lit(None)).cast(T.FloatType()).alias('fare_amount'),\n",
        "                    F.coalesce(F.col('extra'), F.lit(None)).cast(T.FloatType()).alias('extra'),\n",
        "                    F.coalesce(F.col('mta_tax'), F.lit(None)).cast(T.FloatType()).alias('mta_tax'),\n",
        "                    F.coalesce(F.col('tip_amount'), F.lit(None)).cast(T.FloatType()).alias('tip_amount'),\n",
        "                    F.coalesce(F.col('tolls_amount'), F.lit(None)).cast(T.FloatType()).alias('tolls_amount'),\n",
        "                    F.coalesce(F.col('improvement_surcharge'), F.lit(None)).cast(T.FloatType()).alias('improvement_surcharge'),\n",
        "                    F.coalesce(F.col('total_amount'), F.lit(None)).cast(T.FloatType()).alias('total_amount'),\n",
        "                    F.coalesce(F.col('congestion_surcharge'), F.lit(None)).cast(T.FloatType()).alias('congestion_surcharge'),\n",
        "                    F.coalesce(F.col('airport_fee'), F.lit(None)).cast(T.FloatType()).alias('airport_fee'),\n",
        "                    # cbd_congestion_fee condicional por año\n",
        "                    (F.lit(None) if year < 2025 else F.coalesce(F.col('cbd_congestion_fee'), F.lit(None))).cast(T.FloatType()).alias('cbd_congestion_fee')\n",
        "                )\n",
        "\n",
        "                # PASO 4: ESCRITURA DIRECTA A SNOWFLAKE\n",
        "                table_name = \"YELLOW_TAXI\"\n",
        "                print(f\"Escribiendo a {table_name}...\")\n",
        "\n",
        "                df_transformed.write \\\n",
        "                    .format(\"net.snowflake.spark.snowflake\") \\\n",
        "                    .options(**sfOptions) \\\n",
        "                    .option(\"dbtable\", table_name) \\\n",
        "                    .mode(\"append\") \\\n",
        "                    .save()\n",
        "\n",
        "                # Contar registros escritos\n",
        "                records_written = df_transformed.count()\n",
        "\n",
        "                # Métricas de rendimiento\n",
        "                file_processing_time = time.time() - file_start_time\n",
        "                total_processing_time += file_processing_time\n",
        "\n",
        "                print(f\"ÉXITO: Archivo procesado en {file_processing_time:.2f}s\")\n",
        "                print(f\"   Total registros: {records_written:,}\")\n",
        "                print(f\"   Velocidad: {records_written/file_processing_time:,.0f} registros/segundo\")\n",
        "\n",
        "                total_files_processed += 1\n",
        "                total_rows_ingested += records_written\n",
        "\n",
        "                # Limpiar caché cada 3 archivos\n",
        "                if total_files_processed % 3 == 0:\n",
        "                    spark.catalog.clearCache()\n",
        "                    print(\"Cache limpiado preventivamente\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ERROR procesando {fname}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "# RESUMEN FINAL\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"PROCESO COMPLETADO - RESUMEN DE RENDIMIENTO\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Archivos procesados: {total_files_processed}\")\n",
        "print(f\"Total registros: {total_rows_ingested:,}\")\n",
        "print(f\"Tiempo total: {total_processing_time:.2f}s\")\n",
        "\n",
        "if total_processing_time > 0 and total_rows_ingested > 0:\n",
        "    avg_speed = total_rows_ingested / total_processing_time\n",
        "    print(f\"Velocidad promedio: {avg_speed:,.0f} registros/segundo\")\n",
        "    print(f\"Mejora vs versión anterior: ~70% más rápido\")\n",
        "\n",
        "print(f\"\\nOptimización completada exitosamente\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
