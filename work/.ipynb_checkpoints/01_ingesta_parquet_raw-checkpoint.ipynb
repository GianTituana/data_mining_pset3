{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS NECESARIOS PARA PROCESAMIENTO DE DATOS\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Imports para procesamiento de datos cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73706d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIAGN√ìSTICO PASO A PASO ===\n",
      "\n",
      "1Ô∏è‚É£ LIMPIANDO ENTORNO SPARK...\n",
      "‚úÖ Procesos Spark terminados\n",
      "‚úÖ Variable SPARK_LOCAL_IP eliminada\n",
      "‚úÖ Contextos Spark limpiados\n",
      "\n",
      "2Ô∏è‚É£ CONFIGURANDO JARS...\n",
      "‚úÖ JAR encontrado: spark-snowflake_2.12-2.12.0-spark_3.4.jar\n",
      "‚úÖ JAR encontrado: snowflake-jdbc-3.14.4.jar\n",
      "\n",
      "3Ô∏è‚É£ CREANDO SPARK (CONFIGURACI√ìN M√çNIMA)...\n",
      "‚úÖ SPARK CREADO EXITOSAMENTE!\n",
      "‚úÖ Test Spark: 3 registros\n",
      "\n",
      "4Ô∏è‚É£ CONFIGURANDO CREDENCIALES SNOWFLAKE...\n",
      "‚úÖ Configuraciones Snowflake preparadas\n",
      "\n",
      "5Ô∏è‚É£ PROBANDO CONEXIONES SNOWFLAKE...\n",
      "\n",
      "üîç Probando configuraci√≥n: Original\n",
      "   URL: YKFGMFI-GRC01155.snowflakecomputing.com\n",
      "   Usuario: MARTIN\n",
      "   Base de datos: NY_TAXI\n",
      "   Creando DataFrame de prueba...\n",
      "   ‚úÖ DataFrame: 2 registros\n",
      "   Intentando conectar a Snowflake...\n",
      "   üéâ CONEXI√ìN EXITOSA con Original!\n",
      "   üìä Resultado: 1\n",
      "\n",
      "6Ô∏è‚É£ PROBANDO LECTURA DE TABLA REAL...\n",
      "Intentando leer TAXI_ZONE_LOOKUP...\n",
      "üéâ TABLA LE√çDA EXITOSAMENTE!\n",
      "üìä TAXI_ZONE_LOOKUP: 265 registros\n",
      "\n",
      "üìã Muestra de datos:\n",
      "+----------+-------+--------------+------------+\n",
      "|LOCATIONID|BOROUGH|ZONE          |SERVICE_ZONE|\n",
      "+----------+-------+--------------+------------+\n",
      "|1         |EWR    |Newark Airport|EWR         |\n",
      "|2         |Queens |Jamaica Bay   |Boro Zone   |\n",
      "+----------+-------+--------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "7Ô∏è‚É£ LIMPIANDO RECURSOS...\n",
      "‚úÖ Sesi√≥n Spark cerrada\n",
      "\n",
      "üèÅ DIAGN√ìSTICO COMPLETADO\n",
      "‚úÖ Resultado: CONEXI√ìN EXITOSA - Puedes proceder con el c√≥digo principal\n"
     ]
    }
   ],
   "source": [
    "# DIAGN√ìSTICO COMPLETO - SPARK + SNOWFLAKE\n",
    "# Vamos a diagnosticar si son las credenciales o la conexi√≥n Spark\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "print(\"=== DIAGN√ìSTICO PASO A PASO ===\")\n",
    "\n",
    "# PASO 1: Limpiar entorno Spark completamente\n",
    "print(\"\\n1Ô∏è‚É£ LIMPIANDO ENTORNO SPARK...\")\n",
    "\n",
    "# Terminar procesos existentes\n",
    "try:\n",
    "    os.system(\"pkill -f 'java.*spark' 2>/dev/null || true\")\n",
    "    os.system(\"pkill -f pyspark 2>/dev/null || true\")\n",
    "    time.sleep(2)\n",
    "    print(\"‚úÖ Procesos Spark terminados\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è No se pudieron terminar procesos (normal)\")\n",
    "\n",
    "# Limpiar variables de entorno\n",
    "env_vars = ['PYSPARK_GATEWAY_PORT', 'PYSPARK_GATEWAY_SECRET', 'SPARK_LOCAL_IP']\n",
    "for var in env_vars:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "        print(f\"‚úÖ Variable {var} eliminada\")\n",
    "\n",
    "# Limpiar contextos Spark\n",
    "try:\n",
    "    if 'spark' in globals():\n",
    "        spark.stop()\n",
    "        del spark\n",
    "    if 'sc' in globals():\n",
    "        sc.stop()\n",
    "        del sc\n",
    "    SparkSession._instantiatedSession = None\n",
    "    SparkContext._active_spark_context = None\n",
    "    print(\"‚úÖ Contextos Spark limpiados\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Limpieza contexto: {e}\")\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# PASO 2: Configurar JARs\n",
    "print(\"\\n2Ô∏è‚É£ CONFIGURANDO JARS...\")\n",
    "jars_dir = '/home/jovyan/work/jars'\n",
    "spark_jars = f\"{jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar,{jars_dir}/snowflake-jdbc-3.14.4.jar\"\n",
    "\n",
    "# Verificar que los JARs existen\n",
    "jar_files = [\n",
    "    f\"{jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar\",\n",
    "    f\"{jars_dir}/snowflake-jdbc-3.14.4.jar\"\n",
    "]\n",
    "\n",
    "for jar in jar_files:\n",
    "    if os.path.exists(jar):\n",
    "        print(f\"‚úÖ JAR encontrado: {os.path.basename(jar)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå JAR FALTANTE: {jar}\")\n",
    "\n",
    "# PASO 3: Crear Spark con configuraci√≥n ultra-minimalista\n",
    "print(\"\\n3Ô∏è‚É£ CREANDO SPARK (CONFIGURACI√ìN M√çNIMA)...\")\n",
    "\n",
    "try:\n",
    "    # Configuraci√≥n ultra-simple para evitar ConnectionRefusedError\n",
    "    spark_test = SparkSession.builder \\\n",
    "        .appName(f\"DiagnosticoSimple_{int(time.time())}\") \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .config(\"spark.jars\", spark_jars) \\\n",
    "        .config(\"spark.driver.memory\", \"512m\") \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .config(\"spark.driver.host\", \"localhost\") \\\n",
    "        .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "        .config(\"spark.ui.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/tmp/spark-warehouse\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"‚úÖ SPARK CREADO EXITOSAMENTE!\")\n",
    "    \n",
    "    # Test b√°sico de funcionamiento\n",
    "    test_df = spark_test.range(3)\n",
    "    test_count = test_df.count()\n",
    "    print(f\"‚úÖ Test Spark: {test_count} registros\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR CREANDO SPARK: {e}\")\n",
    "    print(f\"   Tipo: {type(e).__name__}\")\n",
    "    print(\"\\nüîß RECOMENDACIONES:\")\n",
    "    print(\"   - Reinicia el kernel: Kernel -> Restart\")\n",
    "    print(\"   - Reinicia el contenedor Docker\")\n",
    "    print(\"   - Verifica memoria disponible\")\n",
    "    \n",
    "    # Intentar diagn√≥stico adicional\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)\n",
    "        java_processes = [line for line in result.stdout.split('\\n') if 'java' in line.lower()]\n",
    "        if java_processes:\n",
    "            print(f\"\\nüîç Procesos Java detectados: {len(java_processes)}\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    raise Exception(\"No se pudo crear Spark\")\n",
    "\n",
    "# PASO 4: Configuraciones Snowflake (m√∫ltiples para probar)\n",
    "print(\"\\n4Ô∏è‚É£ CONFIGURANDO CREDENCIALES SNOWFLAKE...\")\n",
    "\n",
    "# Configuraci√≥n 1: Credenciales originales\n",
    "sf_config_original = {\n",
    "    \"sfURL\": \"YKFGMFI-GRC01155.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"MARTIN\",\n",
    "    \"sfPassword\": \"P7kh2nUSu727FKZ\",\n",
    "    \"sfDatabase\": \"NY_TAXI\",\n",
    "    \"sfSchema\": \"RAW\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "# Configuraci√≥n 2: Con diferentes opciones de formato URL\n",
    "sf_config_alt1 = sf_config_original.copy()\n",
    "sf_config_alt1[\"sfURL\"] = \"https://YKFGMFI-GRC01155.snowflakecomputing.com\"\n",
    "\n",
    "# Configuraci√≥n 3: Con puerto expl√≠cito\n",
    "sf_config_alt2 = sf_config_original.copy()\n",
    "sf_config_alt2[\"sfURL\"] = \"YKFGMFI-GRC01155.snowflakecomputing.com:443\"\n",
    "\n",
    "print(\"‚úÖ Configuraciones Snowflake preparadas\")\n",
    "\n",
    "# PASO 5: Probar conexiones progresivamente\n",
    "print(\"\\n5Ô∏è‚É£ PROBANDO CONEXIONES SNOWFLAKE...\")\n",
    "\n",
    "configs_to_test = [\n",
    "    (\"Original\", sf_config_original),\n",
    "    (\"Con HTTPS\", sf_config_alt1),\n",
    "    (\"Con Puerto\", sf_config_alt2)\n",
    "]\n",
    "\n",
    "conexion_exitosa = False\n",
    "\n",
    "for config_name, sf_config in configs_to_test:\n",
    "    print(f\"\\nüîç Probando configuraci√≥n: {config_name}\")\n",
    "    print(f\"   URL: {sf_config['sfURL']}\")\n",
    "    print(f\"   Usuario: {sf_config['sfUser']}\")\n",
    "    print(f\"   Base de datos: {sf_config['sfDatabase']}\")\n",
    "    \n",
    "    try:\n",
    "        # Test de conexi√≥n b√°sica (sin leer datos)\n",
    "        print(\"   Creando DataFrame de prueba...\")\n",
    "        \n",
    "        test_data = [(1, f\"test_{config_name}\"), (2, \"conexion\")]\n",
    "        test_df = spark_test.createDataFrame(test_data, [\"id\", \"descripcion\"])\n",
    "        print(f\"   ‚úÖ DataFrame: {test_df.count()} registros\")\n",
    "        \n",
    "        # Intentar leer metadatos de Snowflake (m√°s ligero que leer datos)\n",
    "        print(\"   Intentando conectar a Snowflake...\")\n",
    "        \n",
    "        # Usar una consulta muy simple para verificar conectividad\n",
    "        simple_query = \"SELECT 1 as test_conexion\"\n",
    "        \n",
    "        snow_test = spark_test.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sf_config) \\\n",
    "            .option(\"query\", simple_query) \\\n",
    "            .load()\n",
    "        \n",
    "        resultado = snow_test.collect()\n",
    "        print(f\"   üéâ CONEXI√ìN EXITOSA con {config_name}!\")\n",
    "        print(f\"   üìä Resultado: {resultado[0]['TEST_CONEXION']}\")\n",
    "        \n",
    "        conexion_exitosa = True\n",
    "        config_exitosa = sf_config\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_str = str(e)\n",
    "        print(f\"   ‚ùå Error con {config_name}: {type(e).__name__}\")\n",
    "        \n",
    "        # An√°lisis espec√≠fico del error\n",
    "        if \"authentication\" in error_str.lower() or \"credential\" in error_str.lower():\n",
    "            print(\"   üîç PROBLEMA DE CREDENCIALES detectado\")\n",
    "        elif \"network\" in error_str.lower() or \"connection\" in error_str.lower():\n",
    "            print(\"   üîç PROBLEMA DE RED detectado\")\n",
    "        elif \"snowflake\" in error_str.lower():\n",
    "            print(\"   üîç PROBLEMA ESPEC√çFICO DE SNOWFLAKE\")\n",
    "        elif \"connectionrefused\" in error_str.lower():\n",
    "            print(\"   üîç PROBLEMA DE SPARK (no Snowflake)\")\n",
    "        else:\n",
    "            print(f\"   üîç Error desconocido: {error_str[:100]}...\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "# PASO 6: Si hay conexi√≥n exitosa, probar tabla real\n",
    "if conexion_exitosa:\n",
    "    print(f\"\\n6Ô∏è‚É£ PROBANDO LECTURA DE TABLA REAL...\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Intentando leer TAXI_ZONE_LOOKUP...\")\n",
    "        \n",
    "        zone_df = spark_test.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**config_exitosa) \\\n",
    "            .option(\"dbtable\", \"TAXI_ZONE_LOOKUP\") \\\n",
    "            .load()\n",
    "        \n",
    "        zone_count = zone_df.count()\n",
    "        print(f\"üéâ TABLA LE√çDA EXITOSAMENTE!\")\n",
    "        print(f\"üìä TAXI_ZONE_LOOKUP: {zone_count} registros\")\n",
    "        \n",
    "        # Mostrar muestra peque√±a\n",
    "        print(\"\\nüìã Muestra de datos:\")\n",
    "        zone_df.show(2, truncate=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error leyendo tabla: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n‚ùå NINGUNA CONFIGURACI√ìN FUNCION√ì\")\n",
    "    print(\"\\nüîß ACCIONES RECOMENDADAS:\")\n",
    "    print(\"1. Verificar credenciales Snowflake en interfaz web\")\n",
    "    print(\"2. Comprobar que el warehouse COMPUTE_WH est√° activo\")\n",
    "    print(\"3. Verificar permisos del usuario MARTIN\")\n",
    "    print(\"4. Confirmar que la base de datos NY_TAXI existe\")\n",
    "\n",
    "# PASO 7: Limpiar recursos\n",
    "print(f\"\\n7Ô∏è‚É£ LIMPIANDO RECURSOS...\")\n",
    "try:\n",
    "    if 'spark_test' in locals():\n",
    "        spark_test.stop()\n",
    "        print(\"‚úÖ Sesi√≥n Spark cerrada\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error cerrando Spark: {e}\")\n",
    "\n",
    "print(f\"\\nüèÅ DIAGN√ìSTICO COMPLETADO\")\n",
    "\n",
    "if conexion_exitosa:\n",
    "    print(\"‚úÖ Resultado: CONEXI√ìN EXITOSA - Puedes proceder con el c√≥digo principal\")\n",
    "else:\n",
    "    print(\"‚ùå Resultado: PROBLEMAS DE CONEXI√ìN - Revisar credenciales/configuraci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc3252ed-24ca-48bb-b641-11e9dd458bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2826fcd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Descarga datos yellow y green + taxi_zone_lookup para todos los meses de 2015 a 2025\n",
    "start_year = 2019\n",
    "end_year = 2025\n",
    "months = range(1, 13)\n",
    "base_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data'\n",
    "zone_url = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n",
    "data_dir = '/home/jovyan/work/data'  # Carpeta montada por Docker, accesible desde local\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "missing_files = []\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for color in ['yellow', 'green']:\n",
    "        for m in months:\n",
    "            fname = f'{color}_tripdata_{year}-{m:02d}.parquet'\n",
    "            url = f'{base_url}/{fname}'\n",
    "            dest = f'{data_dir}/{fname}'\n",
    "            exit_code = os.system(f'wget -O {dest} {url}')\n",
    "            if exit_code != 0 or not Path(dest).is_file():\n",
    "                missing_files.append(fname)\n",
    "# Descarga taxi_zone_lookup\n",
    "zone_dest = f'{data_dir}/taxi_zone_lookup.csv'\n",
    "exit_code = os.system(f'wget -O {zone_dest} {zone_url}')\n",
    "if exit_code != 0 or not Path(zone_dest).is_file():\n",
    "    missing_files.append('taxi_zone_lookup.csv')\n",
    "\n",
    "# Resumen\n",
    "if missing_files:\n",
    "    print('Faltan los siguientes archivos:')\n",
    "    for f in missing_files:\n",
    "        print('-', f)\n",
    "else:\n",
    "    print('Todos los archivos descargados correctamente.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31895bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snowflake-snowpark-python in /opt/conda/lib/python3.11/site-packages (1.40.0)\n",
      "Requirement already satisfied: snowflake-connector-python in /opt/conda/lib/python3.11/site-packages (3.18.0)\n",
      "Requirement already satisfied: setuptools>=40.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (68.2.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (0.41.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (4.8.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (6.0.1)\n",
      "Requirement already satisfied: cloudpickle!=2.1.0,!=2.2.0,<=3.1.1,>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (3.0.0)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.20 in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (4.24.3)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (2.8.2)\n",
      "Requirement already satisfied: tzlocal in /opt/conda/lib/python3.11/site-packages (from snowflake-snowpark-python) (5.3.1)\n",
      "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.5.1)\n",
      "Requirement already satisfied: boto3>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.55)\n",
      "Requirement already satisfied: botocore>=1.24 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.40.55)\n",
      "Requirement already satisfied: cffi<2.0.0,>=1.9 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.4)\n",
      "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2.0)\n",
      "Requirement already satisfied: pyjwt<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.8.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.3.post1)\n",
      "Requirement already satisfied: requests<3.0.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\n",
      "Requirement already satisfied: filelock<4,>=3.5 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.20.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (2.4.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (3.11.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/lib/python3.11/site-packages (from snowflake-connector-python) (0.13.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.14.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.0.7)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil->snowflake-snowpark-python) (1.16.0)\n",
      "--2025-10-18 23:06:46--  https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.12.0-spark_3.4/spark-snowflake_2.12-2.12.0-spark_3.4.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 104.18.18.12, 104.18.19.12, 2606:4700::6812:130c, ...\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|104.18.18.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 772490 (754K) [application/java-archive]\n",
      "Saving to: ‚Äò/home/jovyan/work/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar‚Äô\n",
      "\n",
      "/home/jovyan/work/j 100%[===================>] 754.38K  2.77MB/s    in 0.3s    \n",
      "\n",
      "2025-10-18 23:06:47 (2.77 MB/s) - ‚Äò/home/jovyan/work/jars/spark-snowflake_2.12-2.12.0-spark_3.4.jar‚Äô saved [772490/772490]\n",
      "\n",
      "--2025-10-18 23:06:47--  https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.14.4/snowflake-jdbc-3.14.4.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 104.18.18.12, 104.18.19.12, 2606:4700::6812:130c, ...\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|104.18.18.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 70552650 (67M) [application/java-archive]\n",
      "Saving to: ‚Äò/home/jovyan/work/jars/snowflake-jdbc-3.14.4.jar‚Äô\n",
      "\n",
      "/home/jovyan/work/j 100%[===================>]  67.28M  2.38MB/s    in 19s     \n",
      "\n",
      "2025-10-18 23:07:07 (3.49 MB/s) - ‚Äò/home/jovyan/work/jars/snowflake-jdbc-3.14.4.jar‚Äô saved [70552650/70552650]\n",
      "\n",
      "‚úì JARs descargados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de dependencias y descarga de JARs para Spark-Snowflake\n",
    "!pip install snowflake-snowpark-python snowflake-connector-python\n",
    "\n",
    "# Crear directorio para JARs si no existe\n",
    "import os\n",
    "jars_dir = '/home/jovyan/work/jars'\n",
    "os.makedirs(jars_dir, exist_ok=True)\n",
    "\n",
    "# Descargar JARs necesarios para Spark 3.x con Scala 2.12\n",
    "# Snowflake Spark Connector compatible con Spark 3.x y Scala 2.12\n",
    "snowflake_jar_url = \"https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.12.0-spark_3.4/spark-snowflake_2.12-2.12.0-spark_3.4.jar\"\n",
    "snowflake_jdbc_url = \"https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.14.4/snowflake-jdbc-3.14.4.jar\"\n",
    "\n",
    "# Descargar JARs\n",
    "!wget -O {jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar {snowflake_jar_url}\n",
    "!wget -O {jars_dir}/snowflake-jdbc-3.14.4.jar {snowflake_jdbc_url}\n",
    "\n",
    "print(\"JARs descargados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80816747",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cerrando sesiones anteriores (normal): [Errno 111] Connection refused\n",
      "Creando nueva sesi√≥n Spark...\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 3. Crear nueva sesi√≥n Spark con configuraci√≥n simplificada\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreando nueva sesi√≥n Spark...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNYC_Taxi_Ingesta_Raw_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark_jars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.maxResultSize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.adaptive.coalescePartitions.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.serializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark.serializer.KryoSerializer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.extraJavaOptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-Dio.netty.tryReflectionSetAccessible=true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.extraJavaOptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-Dio.netty.tryReflectionSetAccessible=true\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 4. Verificar que Spark funciona correctamente\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:493\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m session\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     sparkConf \u001b[38;5;241m=\u001b[39m \u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    495\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/conf.py:132\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    128\u001b[0m _jvm \u001b[38;5;241m=\u001b[39m _jvm \u001b[38;5;129;01mor\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkConf\u001b[49m(loadDefaults)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "# Reinicio completo de Spark para resolver problemas de conexi√≥n\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Limpiar completamente cualquier sesi√≥n Spark existente\n",
    "try:\n",
    "    # Intentar cerrar sesi√≥n existente\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    \n",
    "    # Limpiar contexto de Spark\n",
    "    if 'sc' in locals():\n",
    "        sc.stop()\n",
    "        \n",
    "    # Limpiar variables de entorno problem√°ticas\n",
    "    os.environ.pop('PYSPARK_GATEWAY_PORT', None)\n",
    "    os.environ.pop('PYSPARK_GATEWAY_SECRET', None)\n",
    "    \n",
    "    print(\"Sesiones Spark anteriores cerradas\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cerrando sesiones anteriores (normal): {e}\")\n",
    "\n",
    "# 2. Configuraci√≥n de JARs\n",
    "jars_dir = '/home/jovyan/work/jars'\n",
    "spark_jars = f\"{jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar,{jars_dir}/snowflake-jdbc-3.14.4.jar\"\n",
    "\n",
    "# 3. Crear nueva sesi√≥n Spark con configuraci√≥n simplificada\n",
    "print(\"Creando nueva sesi√≥n Spark...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_Ingesta_Raw_v2\") \\\n",
    "    .config(\"spark.jars\", spark_jars) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 4. Verificar que Spark funciona correctamente\n",
    "try:\n",
    "    test_df = spark.range(10)\n",
    "    count = test_df.count()\n",
    "    print(f\"Spark funcionando correctamente. Test count: {count}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error en test de Spark: {e}\")\n",
    "    raise\n",
    "\n",
    "# 5. Configuraci√≥n Snowflake\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"YKFGMFI-GRC01155.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"MARTIN\",\n",
    "    \"sfPassword\": \"P7kh2nUSu727FKZ\",\n",
    "    \"sfDatabase\": \"NY_TAXI\",\n",
    "    \"sfSchema\": \"RAW\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "# 6. Variables de configuraci√≥n\n",
    "service_types = ['green']\n",
    "start_year = 2015\n",
    "end_year = 2015\n",
    "months = range(2)\n",
    "data_dir = '/home/jovyan/work/data'\n",
    "run_id = f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "ingested_at_utc = datetime.utcnow()\n",
    "auditoria = []\n",
    "\n",
    "# ================================\n",
    "# ESTRATEGIA DE TABLAS ESPEJO Y PARTICIONADO\n",
    "# ================================\n",
    "# DECISI√ìN ARQUITECT√ìNICA: Partici√≥n l√≥gica por a√±o/mes en mismo esquema\n",
    "# \n",
    "# JUSTIFICACI√ìN:\n",
    "# - Una tabla por servicio (GREEN_TAXI, YELLOW_TAXI) para mantener esquemas espec√≠ficos\n",
    "# - Partici√≥n l√≥gica por a√±o/mes usando metadatos (SOURCE_YEAR, SOURCE_MONTH)\n",
    "# - Evita proliferaci√≥n de tablas f√≠sicas (ser√≠a 24 tablas por a√±o por servicio)\n",
    "# - Facilita consultas cross-per√≠odo y mantenimiento\n",
    "# - Snowflake optimiza autom√°ticamente con clustering en columnas temporales\n",
    "#\n",
    "# ESTRUCTURA:\n",
    "# - GREEN_TAXI: contiene todos los a√±os/meses de green taxi\n",
    "# - YELLOW_TAXI: contiene todos los a√±os/meses de yellow taxi  \n",
    "# - TAXI_ZONE_LOOKUP: tabla de referencia\n",
    "# - RAW_AUDITORIA: metadatos completos de todas las cargas\n",
    "#\n",
    "# METADATOS OBLIGATORIOS POR REGISTRO:\n",
    "# - run_id: identificador √∫nico de ejecuci√≥n\n",
    "# - service_type: 'green' o 'yellow'\n",
    "# - source_year, source_month: partici√≥n l√≥gica temporal\n",
    "# - ingested_at_utc: timestamp UTC de ingesta\n",
    "# - source_path: ruta del archivo origen\n",
    "# - batch_number: n√∫mero de lote dentro del archivo\n",
    "# - natural_key: clave de negocio para idempotencia\n",
    "#\n",
    "# IDEMPOTENCIA:\n",
    "# - Clave natural: pickup_datetime + pulocationid + dolocationid + vendorid\n",
    "# - Upsert strategy: DELETE + INSERT por (service_type, source_year, source_month)\n",
    "# ================================\n",
    "\n",
    "print(f\"Configuraci√≥n completada - Run ID: {run_id}\")\n",
    "print(f\"Estrategia: Partici√≥n l√≥gica por a√±o/mes en tablas por servicio\")\n",
    "\n",
    "# 7. Funci√≥n MEJORADA para carga idempotente a Snowflake con metadatos completos\n",
    "def load_to_snowflake_idempotent(sdf, table_name, service_type, source_year, source_month, sfOptions, batch_size=1000000):\n",
    "    \"\"\"\n",
    "    Carga idempotente a Snowflake con estrategia de reemplazo por partici√≥n l√≥gica\n",
    "    \n",
    "    ESTRATEGIA DE IDEMPOTENCIA:\n",
    "    1. Eliminar registros existentes de la misma partici√≥n (service_type + year + month)\n",
    "    2. Insertar nuevos registros con metadatos completos\n",
    "    3. Registrar auditor√≠a detallada por lote\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PASO 1: Preparar DataFrame con metadatos obligatorios\n",
    "        print(f\"üîß Preparando metadatos para {table_name} - {service_type} {source_year}-{source_month:02d}\")\n",
    "        \n",
    "        sdf_cached = sdf.cache()\n",
    "        total_rows = sdf_cached.count()\n",
    "        \n",
    "        print(f\"üìä Registros a procesar: {total_rows:,}\")\n",
    "        \n",
    "        if total_rows == 0:\n",
    "            print(\"‚ö†Ô∏è No hay registros para procesar\")\n",
    "            return 0, []\n",
    "        \n",
    "        # PASO 2: Implementar estrategia de idempotencia\n",
    "        print(f\"üóëÔ∏è Eliminando registros existentes para {service_type} {source_year}-{source_month:02d}\")\n",
    "        \n",
    "        delete_query = f\"\"\"\n",
    "        DELETE FROM {table_name} \n",
    "        WHERE SERVICE_TYPE = '{service_type}' \n",
    "        AND SOURCE_YEAR = {source_year} \n",
    "        AND SOURCE_MONTH = {source_month}\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Ejecutar DELETE para idempotencia\n",
    "            delete_df = spark.read \\\n",
    "                .format(\"net.snowflake.spark.snowflake\") \\\n",
    "                .options(**sfOptions) \\\n",
    "                .option(\"query\", delete_query) \\\n",
    "                .load()\n",
    "            \n",
    "            # Trigger execution\n",
    "            delete_count = delete_df.count()\n",
    "            print(f\"‚úÖ Limpieza completada (query ejecutada)\")\n",
    "            \n",
    "        except Exception as delete_error:\n",
    "            print(f\"‚ö†Ô∏è Error en DELETE (puede ser normal si no hay datos previos): {delete_error}\")\n",
    "        \n",
    "        # PASO 3: Procesar en lotes con metadatos por lote\n",
    "        if total_rows <= batch_size:\n",
    "            print(f\"üì¶ Procesando como lote √∫nico...\")\n",
    "            \n",
    "            # Agregar metadatos del lote\n",
    "            sdf_with_metadata = sdf_cached \\\n",
    "                .withColumn('BATCH_NUMBER', F.lit(1)) \\\n",
    "                .withColumn('BATCH_SIZE', F.lit(total_rows)) \\\n",
    "                .withColumn('TOTAL_FILE_ROWS', F.lit(total_rows))\n",
    "            \n",
    "            # Cargar\n",
    "            sdf_with_metadata.write \\\n",
    "                .format(\"net.snowflake.spark.snowflake\") \\\n",
    "                .options(**sfOptions) \\\n",
    "                .option(\"dbtable\", table_name) \\\n",
    "                .option(\"truncate_table\", \"off\") \\\n",
    "                .option(\"usestagingtable\", \"on\") \\\n",
    "                .option(\"continue_on_error\", \"false\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .save()\n",
    "            \n",
    "            batch_audit = [{\n",
    "                'batch_number': 1,\n",
    "                'batch_rows': total_rows,\n",
    "                'batch_start_time': datetime.utcnow(),\n",
    "                'batch_end_time': datetime.utcnow()\n",
    "            }]\n",
    "            \n",
    "            sdf_cached.unpersist()\n",
    "            print(f\"‚úÖ {total_rows:,} filas cargadas en 1 lote\")\n",
    "            return total_rows, batch_audit\n",
    "            \n",
    "        else:\n",
    "            # Procesamiento en m√∫ltiples lotes\n",
    "            print(f\"üì¶ Procesando en lotes de {batch_size:,} filas...\")\n",
    "            \n",
    "            num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "            print(f\"Total de lotes: {num_batches}\")\n",
    "            \n",
    "            # Repartir para distribuci√≥n uniforme\n",
    "            sdf_repartitioned = sdf_cached.repartition(num_batches)\n",
    "            \n",
    "            total_loaded = 0\n",
    "            batch_audit = []\n",
    "            \n",
    "            # Procesar particiones como lotes\n",
    "            partitions = sdf_repartitioned.rdd.glom().collect()\n",
    "            \n",
    "            for batch_num, partition_data in enumerate(partitions, 1):\n",
    "                if not partition_data:\n",
    "                    continue\n",
    "                \n",
    "                batch_start = datetime.utcnow()\n",
    "                batch_rows = len(partition_data)\n",
    "                \n",
    "                print(f\"  üì¶ Lote {batch_num}/{len(partitions)}: {batch_rows:,} filas\")\n",
    "                \n",
    "                # Crear DataFrame con metadatos de lote\n",
    "                partition_df = spark.createDataFrame(partition_data, sdf_cached.schema)\n",
    "                \n",
    "                partition_with_metadata = partition_df \\\n",
    "                    .withColumn('BATCH_NUMBER', F.lit(batch_num)) \\\n",
    "                    .withColumn('BATCH_SIZE', F.lit(batch_rows)) \\\n",
    "                    .withColumn('TOTAL_FILE_ROWS', F.lit(total_rows))\n",
    "                \n",
    "                # Cargar lote\n",
    "                partition_with_metadata.write \\\n",
    "                    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "                    .options(**sfOptions) \\\n",
    "                    .option(\"dbtable\", table_name) \\\n",
    "                    .option(\"truncate_table\", \"off\") \\\n",
    "                    .option(\"usestagingtable\", \"on\") \\\n",
    "                    .option(\"continue_on_error\", \"false\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save()\n",
    "                \n",
    "                batch_end = datetime.utcnow()\n",
    "                total_loaded += batch_rows\n",
    "                \n",
    "                # Registrar auditor√≠a del lote\n",
    "                batch_audit.append({\n",
    "                    'batch_number': batch_num,\n",
    "                    'batch_rows': batch_rows,\n",
    "                    'batch_start_time': batch_start,\n",
    "                    'batch_end_time': batch_end\n",
    "                })\n",
    "                \n",
    "                print(f\"    ‚úÖ Lote {batch_num} completado en {(batch_end - batch_start).total_seconds():.1f}s\")\n",
    "            \n",
    "            sdf_cached.unpersist()\n",
    "            print(f\"üéâ CARGA COMPLETA: {total_loaded:,} filas en {len(partitions)} lotes\")\n",
    "            return total_loaded, batch_audit\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_str = str(e)\n",
    "        print(f\"‚ùå Error cargando a {table_name}: {error_str}\")\n",
    "        \n",
    "        # Limpiar cache en caso de error\n",
    "        try:\n",
    "            sdf.unpersist()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return 0, []\n",
    "\n",
    "# 8. Procesamiento por servicio, a√±o, mes\n",
    "total_files_processed = 0\n",
    "total_rows_ingested = 0\n",
    "\n",
    "for service_type in service_types:\n",
    "    print(f\"\\nProcesando servicio: {service_type}\")\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in months:\n",
    "            fname = f'{service_type}_tripdata_{year}-{month+1:02d}.parquet'\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            \n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"Archivo no encontrado: {fname}\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Procesando {fname}\")\n",
    "            \n",
    "            try:\n",
    "                # Leer Parquet\n",
    "                sdf = spark.read.parquet(fpath)\n",
    "                \n",
    "                # DIAGN√ìSTICO: Verificar datos originales\n",
    "                original_count = sdf.count()\n",
    "                original_columns = len(sdf.columns)\n",
    "                print(f\"   üìä ARCHIVO ORIGINAL: {original_count:,} filas, {original_columns} columnas\")\n",
    "                \n",
    "                # Mostrar esquema para debug\n",
    "                print(f\"   üìã Columnas originales: {sdf.columns[:10]}...\")  # Primeras 10\n",
    "                \n",
    "                # Cache para m√∫ltiples operaciones\n",
    "                sdf = sdf.cache()\n",
    "                \n",
    "                # Normalizar columnas timestamp CON VALIDACI√ìN MEJORADA Y PRESERVACI√ìN DE DATOS\n",
    "                print(f\"   üîß Normalizando timestamps con preservaci√≥n de datos...\")\n",
    "                \n",
    "                # CORRECCI√ìN CR√çTICA: Preservar los valores originales antes de transformar\n",
    "                timestamp_mapping = {\n",
    "                    'tpep_pickup_datetime': 'TPEP_PICKUP_DATETIME',\n",
    "                    'tpep_dropoff_datetime': 'TPEP_DROPOFF_DATETIME', \n",
    "                    'lpep_pickup_datetime': 'LPEP_PICKUP_DATETIME',\n",
    "                    'lpep_dropoff_datetime': 'LPEP_DROPOFF_DATETIME'\n",
    "                }\n",
    "                \n",
    "                for old_col, new_col in timestamp_mapping.items():\n",
    "                    if old_col in sdf.columns:\n",
    "                        current_type = sdf.schema[old_col].dataType\n",
    "                        print(f\"   üïê Procesando {old_col} (tipo: {current_type})\")\n",
    "                        \n",
    "                        # Mostrar muestra de datos ANTES de transformar\n",
    "                        sample_data = sdf.select(old_col).limit(3).collect()\n",
    "                        print(f\"   üìä Muestra ANTES: {[row[old_col] for row in sample_data]}\")\n",
    "                        \n",
    "                        # Solo convertir si hay datos v√°lidos\n",
    "                        if not isinstance(current_type, T.NullType):\n",
    "                            if isinstance(current_type, T.LongType):\n",
    "                                # Epoch en milisegundos a timestamp\n",
    "                                print(f\"   üîÑ Convirtiendo desde epoch milisegundos\")\n",
    "                                sdf = sdf.withColumn(new_col, \n",
    "                                    F.when(F.col(old_col).isNull(), None)\n",
    "                                     .when(F.col(old_col) == 0, None)\n",
    "                                     .otherwise(F.from_unixtime(F.col(old_col) / 1000).cast(T.TimestampType())))\n",
    "                                     \n",
    "                            elif isinstance(current_type, T.StringType):\n",
    "                                # String a timestamp con m√∫ltiples formatos\n",
    "                                print(f\"   üîÑ Convirtiendo desde string\")\n",
    "                                sdf = sdf.withColumn(new_col,\n",
    "                                    F.when(F.col(old_col).isNull(), None)\n",
    "                                     .when(F.col(old_col) == \"\", None)\n",
    "                                     .otherwise(F.to_timestamp(F.col(old_col))))\n",
    "                                     \n",
    "                            elif isinstance(current_type, T.TimestampType):\n",
    "                                # Ya es timestamp, solo renombrar si es necesario\n",
    "                                print(f\"   ‚úÖ Ya es timestamp, renombrando\")\n",
    "                                if old_col != new_col:\n",
    "                                    sdf = sdf.withColumnRenamed(old_col, new_col)\n",
    "                                else:\n",
    "                                    # No hacer nada, ya est√° correcto\n",
    "                                    pass\n",
    "                            else:\n",
    "                                # Conversi√≥n directa para otros tipos\n",
    "                                print(f\"   üîÑ Conversi√≥n directa desde {current_type}\")\n",
    "                                sdf = sdf.withColumn(new_col, F.col(old_col).cast(T.TimestampType()))\n",
    "                        else:\n",
    "                            # NullType - crear columna timestamp vac√≠a\n",
    "                            print(f\"   ‚ö†Ô∏è Columna {old_col} es NullType, creando timestamp vac√≠o\")\n",
    "                            sdf = sdf.withColumn(new_col, F.lit(None).cast(T.TimestampType()))\n",
    "                        \n",
    "                        # Verificar conversi√≥n\n",
    "                        if new_col in sdf.columns:\n",
    "                            sample_converted = sdf.select(new_col).limit(3).collect()\n",
    "                            print(f\"   üìä Muestra DESPU√âS: {[row[new_col] for row in sample_converted]}\")\n",
    "                        \n",
    "                        # Eliminar columna original si es diferente\n",
    "                        if old_col != new_col and old_col in sdf.columns:\n",
    "                            sdf = sdf.drop(old_col)\n",
    "                \n",
    "                # Normalizar VendorID\n",
    "                if 'VendorID' in sdf.columns:\n",
    "                    sdf = sdf.withColumnRenamed('VendorID', 'VENDORID')\n",
    "                if 'VENDORID' in sdf.columns:\n",
    "                    sdf = sdf.withColumn('VENDORID', F.col('VENDORID').cast(T.IntegerType()))\n",
    "                \n",
    "                # Agregar metadatos OBLIGATORIOS COMPLETOS\n",
    "                print(f\"   üìã Agregando metadatos obligatorios...\")\n",
    "                sdf = sdf.withColumn('RUN_ID', F.lit(run_id)) \\\n",
    "                         .withColumn('SERVICE_TYPE', F.lit(service_type)) \\\n",
    "                         .withColumn('SOURCE_YEAR', F.lit(year)) \\\n",
    "                         .withColumn('SOURCE_MONTH', F.lit(month+1)) \\\n",
    "                         .withColumn('INGESTED_AT_UTC', F.lit(ingested_at_utc)) \\\n",
    "                         .withColumn('SOURCE_PATH', F.lit(fpath)) \\\n",
    "                         .withColumn('FILE_SIZE_BYTES', F.lit(os.path.getsize(fpath))) \\\n",
    "                         .withColumn('ORIGINAL_FILE_ROWS', F.lit(original_count))\n",
    "                \n",
    "                # Crear CLAVE NATURAL ROBUSTA para idempotencia\n",
    "                print(f\"   üîë Creando clave natural robusta...\")\n",
    "                \n",
    "                # Identificar columna pickup principal\n",
    "                pickup_col = None\n",
    "                dropoff_col = None\n",
    "                \n",
    "                if service_type == 'green':\n",
    "                    pickup_col = 'LPEP_PICKUP_DATETIME'\n",
    "                    dropoff_col = 'LPEP_DROPOFF_DATETIME'\n",
    "                elif service_type == 'yellow':\n",
    "                    pickup_col = 'TPEP_PICKUP_DATETIME'\n",
    "                    dropoff_col = 'TPEP_DROPOFF_DATETIME'\n",
    "                \n",
    "                # Construir clave natural con componentes disponibles\n",
    "                natural_key_components = []\n",
    "                \n",
    "                # 1. Timestamp pickup (obligatorio para clave v√°lida)\n",
    "                if pickup_col and pickup_col in sdf.columns:\n",
    "                    natural_key_components.append(F.coalesce(F.col(pickup_col).cast('string'), F.lit('NULL_PICKUP')))\n",
    "                else:\n",
    "                    natural_key_components.append(F.lit('NO_PICKUP'))\n",
    "                \n",
    "                # 2. Ubicaciones (si est√°n disponibles)\n",
    "                if 'PULOCATIONID' in sdf.columns:\n",
    "                    natural_key_components.append(F.coalesce(F.col('PULOCATIONID').cast('string'), F.lit('NULL_PU')))\n",
    "                else:\n",
    "                    natural_key_components.append(F.lit('NO_PU'))\n",
    "                    \n",
    "                if 'DOLOCATIONID' in sdf.columns:\n",
    "                    natural_key_components.append(F.coalesce(F.col('DOLOCATIONID').cast('string'), F.lit('NULL_DO')))\n",
    "                else:\n",
    "                    natural_key_components.append(F.lit('NO_DO'))\n",
    "                \n",
    "                # 3. VendorID (si est√° disponible)\n",
    "                if 'VENDORID' in sdf.columns:\n",
    "                    natural_key_components.append(F.coalesce(F.col('VENDORID').cast('string'), F.lit('NULL_VENDOR')))\n",
    "                else:\n",
    "                    natural_key_components.append(F.lit('NO_VENDOR'))\n",
    "                \n",
    "                # 4. Distancia del viaje (como diferenciador adicional)\n",
    "                if 'TRIP_DISTANCE' in sdf.columns:\n",
    "                    natural_key_components.append(F.coalesce(F.col('TRIP_DISTANCE').cast('string'), F.lit('NULL_DIST')))\n",
    "                else:\n",
    "                    natural_key_components.append(F.lit('NO_DIST'))\n",
    "                \n",
    "                # Crear clave natural concatenada\n",
    "                sdf = sdf.withColumn('NATURAL_KEY', F.concat_ws('|', *natural_key_components))\n",
    "                \n",
    "                # Verificar calidad de la clave natural\n",
    "                sample_keys = sdf.select('NATURAL_KEY').limit(3).collect()\n",
    "                print(f\"   üîç Ejemplos clave natural: {[row['NATURAL_KEY'] for row in sample_keys]}\")\n",
    "                \n",
    "                # Contar claves nulas o problem√°ticas\n",
    "                null_keys = sdf.filter(F.col('NATURAL_KEY').isNull() | (F.col('NATURAL_KEY') == '')).count()\n",
    "                print(f\"   üìä Claves naturales problem√°ticas: {null_keys:,}\")\n",
    "                \n",
    "                if null_keys > 0:\n",
    "                    print(f\"   ‚ö†Ô∏è Hay {null_keys:,} registros con claves naturales problem√°ticas\")\n",
    "                \n",
    "                # Eliminar duplicados\n",
    "                if 'NATURAL_KEY' in sdf.columns:\n",
    "                    # DIAGN√ìSTICO: Contar antes de eliminar duplicados\n",
    "                    count_before_dedup = sdf.count()\n",
    "                    print(f\"   üìä ANTES de eliminar duplicados: {count_before_dedup:,} filas\")\n",
    "                    \n",
    "                    sdf = sdf.dropDuplicates(['NATURAL_KEY'])\n",
    "                    \n",
    "                    # DIAGN√ìSTICO: Contar despu√©s de eliminar duplicados\n",
    "                    count_after_dedup = sdf.count()\n",
    "                    duplicates_removed = count_before_dedup - count_after_dedup\n",
    "                    print(f\"   üìä DESPU√âS de eliminar duplicados: {count_after_dedup:,} filas\")\n",
    "                    print(f\"   üóëÔ∏è Duplicados eliminados: {duplicates_removed:,} filas\")\n",
    "                    \n",
    "                    if duplicates_removed > (count_before_dedup * 0.8):  # Si se eliminan m√°s del 80%\n",
    "                        print(f\"   ‚ö†Ô∏è ADVERTENCIA: Se eliminaron demasiados duplicados ({duplicates_removed/count_before_dedup*100:.1f}%)\")\n",
    "                        \n",
    "                        # Verificar calidad de NATURAL_KEY\n",
    "                        null_keys = sdf.filter(F.col('NATURAL_KEY').isNull()).count()\n",
    "                        print(f\"   üîç NATURAL_KEY nulos: {null_keys:,}\")\n",
    "                        \n",
    "                        # Mostrar algunos ejemplos de NATURAL_KEY\n",
    "                        sample_keys = sdf.select('NATURAL_KEY').limit(5).collect()\n",
    "                        print(f\"   üîç Ejemplos NATURAL_KEY: {[row['NATURAL_KEY'] for row in sample_keys]}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è Sin columna NATURAL_KEY para deduplicar\")\n",
    "                \n",
    "                # Normalizar nombres a may√∫sculas\n",
    "                for old_col in sdf.columns:\n",
    "                    new_col = old_col.upper()\n",
    "                    if old_col != new_col:\n",
    "                        sdf = sdf.withColumnRenamed(old_col, new_col)\n",
    "                \n",
    "                # AGREGAR COLUMNAS FALTANTES PARA COMPATIBILIDAD - SCHEMA MEJORADO\n",
    "                print(f\"   üìã Normalizando esquema con metadatos completos...\")\n",
    "                \n",
    "                # Esquemas mejorados con metadatos obligatorios\n",
    "                expected_columns_green = [\n",
    "                    # Metadatos obligatorios (primero para mejor organizaci√≥n)\n",
    "                    'RUN_ID', 'SERVICE_TYPE', 'SOURCE_YEAR', 'SOURCE_MONTH', \n",
    "                    'INGESTED_AT_UTC', 'SOURCE_PATH', 'NATURAL_KEY',\n",
    "                    'FILE_SIZE_BYTES', 'ORIGINAL_FILE_ROWS', 'BATCH_NUMBER', 'BATCH_SIZE', 'TOTAL_FILE_ROWS',\n",
    "                    \n",
    "                    # Datos de negocio Green Taxi\n",
    "                    'VENDORID', 'LPEP_PICKUP_DATETIME', 'LPEP_DROPOFF_DATETIME',\n",
    "                    'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATECODEID', \n",
    "                    'STORE_AND_FWD_FLAG', 'PULOCATIONID', 'DOLOCATIONID',\n",
    "                    'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX',\n",
    "                    'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE',\n",
    "                    'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE', 'CBD_CONGESTION_FEE'\n",
    "                ]\n",
    "                \n",
    "                expected_columns_yellow = [\n",
    "                    # Metadatos obligatorios (primero para mejor organizaci√≥n)\n",
    "                    'RUN_ID', 'SERVICE_TYPE', 'SOURCE_YEAR', 'SOURCE_MONTH',\n",
    "                    'INGESTED_AT_UTC', 'SOURCE_PATH', 'NATURAL_KEY',\n",
    "                    'FILE_SIZE_BYTES', 'ORIGINAL_FILE_ROWS', 'BATCH_NUMBER', 'BATCH_SIZE', 'TOTAL_FILE_ROWS',\n",
    "                    \n",
    "                    # Datos de negocio Yellow Taxi\n",
    "                    'VENDORID', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME',\n",
    "                    'PASSENGER_COUNT', 'TRIP_DISTANCE', 'RATECODEID',\n",
    "                    'STORE_AND_FWD_FLAG', 'PULOCATIONID', 'DOLOCATIONID',\n",
    "                    'PAYMENT_TYPE', 'FARE_AMOUNT', 'EXTRA', 'MTA_TAX',\n",
    "                    'TIP_AMOUNT', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE',\n",
    "                    'TOTAL_AMOUNT', 'CONGESTION_SURCHARGE', 'AIRPORT_FEE'\n",
    "                ]\n",
    "                \n",
    "                # Seleccionar columnas esperadas seg√∫n el tipo de servicio\n",
    "                expected_columns = expected_columns_green if service_type == 'green' else expected_columns_yellow\n",
    "                \n",
    "                # Agregar columnas faltantes con valores apropiados por tipo\n",
    "                current_columns = set(sdf.columns)\n",
    "                for col in expected_columns:\n",
    "                    if col not in current_columns:\n",
    "                        print(f\"   ‚ûï Agregando columna faltante: {col}\")\n",
    "                        \n",
    "                        # Valores por defecto apropiados seg√∫n el tipo de columna\n",
    "                        if col in ['BATCH_NUMBER', 'BATCH_SIZE', 'TOTAL_FILE_ROWS']:\n",
    "                            # Estas se agregar√°n en la funci√≥n de carga\n",
    "                            sdf = sdf.withColumn(col, F.lit(None).cast(T.IntegerType()))\n",
    "                        elif col in ['FILE_SIZE_BYTES', 'ORIGINAL_FILE_ROWS']:\n",
    "                            # Ya se agregaron antes\n",
    "                            continue\n",
    "                        elif 'DATETIME' in col or 'TIMESTAMP' in col:\n",
    "                            sdf = sdf.withColumn(col, F.lit(None).cast(T.TimestampType()))\n",
    "                        elif col in ['FARE_AMOUNT', 'TRIP_DISTANCE', 'TIP_AMOUNT', 'TOTAL_AMOUNT', \n",
    "                                   'EXTRA', 'MTA_TAX', 'TOLLS_AMOUNT', 'IMPROVEMENT_SURCHARGE',\n",
    "                                   'CONGESTION_SURCHARGE', 'AIRPORT_FEE', 'CBD_CONGESTION_FEE']:\n",
    "                            sdf = sdf.withColumn(col, F.lit(None).cast(T.DoubleType()))\n",
    "                        elif col in ['VENDORID', 'PASSENGER_COUNT', 'RATECODEID', 'PULOCATIONID', \n",
    "                                   'DOLOCATIONID', 'PAYMENT_TYPE']:\n",
    "                            sdf = sdf.withColumn(col, F.lit(None).cast(T.IntegerType()))\n",
    "                        else:\n",
    "                            sdf = sdf.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "                \n",
    "                # Reordenar columnas seg√∫n el orden esperado y verificar existencia\n",
    "                existing_columns = [col for col in expected_columns if col in sdf.columns]\n",
    "                missing_columns = [col for col in expected_columns if col not in sdf.columns]\n",
    "                \n",
    "                if missing_columns:\n",
    "                    print(f\"   ‚ö†Ô∏è Columnas a√∫n faltantes: {missing_columns}\")\n",
    "                    # Agregar columnas faltantes como string por defecto\n",
    "                    for col in missing_columns:\n",
    "                        sdf = sdf.withColumn(col, F.lit(None).cast(T.StringType()))\n",
    "                \n",
    "                # Seleccionar en el orden correcto\n",
    "                sdf = sdf.select(*expected_columns)\n",
    "                \n",
    "                # DIAGN√ìSTICO: Verificar despu√©s de reordenar columnas\n",
    "                count_after_reorder = sdf.count()\n",
    "                print(f\"   üìä DESPU√âS de reordenar columnas: {count_after_reorder:,} filas, {len(sdf.columns)} columnas\")\n",
    "                \n",
    "                if count_after_reorder == 0:\n",
    "                    print(f\"   ‚ùå ERROR: Se perdieron todas las filas en reordenamiento\")\n",
    "                    print(f\"   üîç Columnas esperadas: {expected_columns[:5]}...\")\n",
    "                    print(f\"   üîç Columnas actuales antes: {current_columns}\")\n",
    "                    continue\n",
    "                \n",
    "                # VALIDAR Y CORREGIR TIPOS DE DATOS PROBLEM√ÅTICOS\n",
    "                print(f\"   Validando tipos de datos...\")\n",
    "                \n",
    "                # Definir tipos esperados para columnas cr√≠ticas\n",
    "                column_types = {\n",
    "                    'VENDORID': T.IntegerType(),\n",
    "                    'LPEP_PICKUP_DATETIME': T.TimestampType(),\n",
    "                    'LPEP_DROPOFF_DATETIME': T.TimestampType(),\n",
    "                    'TPEP_PICKUP_DATETIME': T.TimestampType(),\n",
    "                    'TPEP_DROPOFF_DATETIME': T.TimestampType(),\n",
    "                    'PASSENGER_COUNT': T.IntegerType(),\n",
    "                    'TRIP_DISTANCE': T.DoubleType(),\n",
    "                    'RATECODEID': T.IntegerType(),\n",
    "                    'PULOCATIONID': T.IntegerType(),\n",
    "                    'DOLOCATIONID': T.IntegerType(),\n",
    "                    'PAYMENT_TYPE': T.IntegerType(),\n",
    "                    'FARE_AMOUNT': T.DoubleType(),\n",
    "                    'EXTRA': T.DoubleType(),\n",
    "                    'MTA_TAX': T.DoubleType(),\n",
    "                    'TIP_AMOUNT': T.DoubleType(),\n",
    "                    'TOLLS_AMOUNT': T.DoubleType(),\n",
    "                    'IMPROVEMENT_SURCHARGE': T.DoubleType(),\n",
    "                    'TOTAL_AMOUNT': T.DoubleType(),\n",
    "                    'CONGESTION_SURCHARGE': T.DoubleType(),\n",
    "                    'AIRPORT_FEE': T.DoubleType(),\n",
    "                    'CBD_CONGESTION_FEE': T.DoubleType()\n",
    "                }\n",
    "                \n",
    "                # Corregir tipos de datos problem√°ticos\n",
    "                for col_name, expected_type in column_types.items():\n",
    "                    if col_name in sdf.columns:\n",
    "                        current_type = sdf.schema[col_name].dataType\n",
    "                        \n",
    "                        # Si es NullType o tipo incorrecto, convertir\n",
    "                        if isinstance(current_type, T.NullType) or current_type != expected_type:\n",
    "                            print(f\"   Corrigiendo tipo {col_name}: {current_type} -> {expected_type}\")\n",
    "                            \n",
    "                            # Para timestamps, manejar casos especiales\n",
    "                            if isinstance(expected_type, T.TimestampType):\n",
    "                                sdf = sdf.withColumn(col_name, \n",
    "                                    F.when(F.col(col_name).isNull(), None)\n",
    "                                     .otherwise(F.col(col_name).cast(expected_type)))\n",
    "                            else:\n",
    "                                sdf = sdf.withColumn(col_name, F.col(col_name).cast(expected_type))\n",
    "                \n",
    "                print(f\"   DataFrame ajustado: {len(sdf.columns)} columnas, tipos validados\")\n",
    "                \n",
    "                # VERIFICAR TIMESTAMPS ANTES DE CARGAR (OPTIMIZADO)\n",
    "                print(f\"   Verificando conversi√≥n de timestamps...\")\n",
    "                \n",
    "                # DIAGN√ìSTICO ESPECIAL: Si hay muy pocas filas, mostrar todo\n",
    "                final_count = sdf.count()\n",
    "                print(f\"   üìä CONTEO FINAL antes de cargar: {final_count:,} filas\")\n",
    "                \n",
    "                if final_count <= 10:\n",
    "                    print(f\"   üîç ARCHIVO PEQUE√ëO - Mostrando todas las filas:\")\n",
    "                    sdf.show(truncate=False)\n",
    "                    \n",
    "                    print(f\"   üîç Esquema final:\")\n",
    "                    sdf.printSchema()\n",
    "                \n",
    "                # Solo verificar si hay columnas timestamp para debug ligero\n",
    "                ts_columns = [col for col in ['LPEP_PICKUP_DATETIME', 'LPEP_DROPOFF_DATETIME', 'TPEP_PICKUP_DATETIME', 'TPEP_DROPOFF_DATETIME'] if col in sdf.columns]\n",
    "                if ts_columns:\n",
    "                    print(f\"   Columnas timestamp presentes: {ts_columns}\")\n",
    "                    # Verificaci√≥n r√°pida: solo contar registros no nulos (m√°s eficiente que collect)\n",
    "                    for ts_col in ts_columns[:2]:  # Solo verificar las primeras 2\n",
    "                        non_null_count = sdf.filter(F.col(ts_col).isNotNull()).count()\n",
    "                        print(f\"   {ts_col}: {non_null_count:,} valores no nulos\")\n",
    "                else:\n",
    "                    print(f\"   Sin columnas timestamp v√°lidas\")\n",
    "                \n",
    "                # Tabla destino\n",
    "                table_name = \"YELLOW_TAXI\" if service_type == 'yellow' else \"GREEN_TAXI\"\n",
    "                \n",
    "                # CARGA IDEMPOTENTE con metadatos completos\n",
    "                print(f\"   üöÄ Iniciando carga idempotente a {table_name}\")\n",
    "                rows_loaded, batch_audit = load_to_snowflake_idempotent(\n",
    "                    sdf, table_name, service_type, year, month+1, sfOptions\n",
    "                )\n",
    "                \n",
    "                if rows_loaded > 0:\n",
    "                    total_files_processed += 1\n",
    "                    total_rows_ingested += rows_loaded\n",
    "                    \n",
    "                    # Auditor√≠a COMPLETA con metadatos de lotes\n",
    "                    for batch_info in batch_audit:\n",
    "                        auditoria.append({\n",
    "                            'table_name': table_name,\n",
    "                            'rows_ingested': batch_info['batch_rows'],\n",
    "                            'batch_number': batch_info['batch_number'],\n",
    "                            'batch_start_time': batch_info['batch_start_time'],\n",
    "                            'batch_end_time': batch_info['batch_end_time'],\n",
    "                            'run_id': run_id,\n",
    "                            'service_type': service_type,\n",
    "                            'source_year': year,\n",
    "                            'source_month': month+1,\n",
    "                            'ingested_at_utc': ingested_at_utc,\n",
    "                            'source_path': fpath,\n",
    "                            'file_size_bytes': os.path.getsize(fpath),\n",
    "                            'original_file_rows': original_count,\n",
    "                            'natural_key_strategy': f'{pickup_col}|PULOCATIONID|DOLOCATIONID|VENDORID|TRIP_DISTANCE'\n",
    "                        })\n",
    "                    \n",
    "                    print(f\"   ‚úÖ {rows_loaded:,} filas cargadas con {len(batch_audit)} lotes auditados\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå No se cargaron filas para {fname}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {fname}: {str(e)}\")\n",
    "                # Limpiar cache en caso de error\n",
    "                try:\n",
    "                    sdf.unpersist()\n",
    "                except:\n",
    "                    pass\n",
    "                continue\n",
    "            \n",
    "            finally:\n",
    "                # OPTIMIZACI√ìN: Siempre liberar memoria del cache\n",
    "                try:\n",
    "                    sdf.unpersist()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(f\"\\nRESUMEN:\")\n",
    "print(f\"Archivos procesados: {total_files_processed}\")\n",
    "print(f\"Total filas: {total_rows_ingested:,}\")\n",
    "\n",
    "# 9. Cargar taxi_zone_lookup\n",
    "print(\"\\nVerificando taxi_zone_lookup...\")\n",
    "try:\n",
    "    zone_check = spark.read \\\n",
    "        .format(\"net.snowflake.spark.snowflake\") \\\n",
    "        .options(**sfOptions) \\\n",
    "        .option(\"query\", \"SELECT COUNT(*) as cnt FROM TAXI_ZONE_LOOKUP\") \\\n",
    "        .load()\n",
    "    \n",
    "    zone_count = zone_check.collect()[0]['cnt']\n",
    "    print(f\"TAXI_ZONE_LOOKUP tiene {zone_count} registros\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error verificando taxi_zone_lookup: {str(e)}\")\n",
    "\n",
    "# 10. Registrar auditor√≠a COMPLETA con metadatos de lotes\n",
    "if auditoria:\n",
    "    print(f\"\\nüìã Registrando auditor√≠a completa ({len(auditoria)} entradas de lotes)...\")\n",
    "    try:\n",
    "        audit_df = spark.createDataFrame(auditoria)\n",
    "        \n",
    "        # Renombrar columnas para tabla de auditor√≠a\n",
    "        column_mapping = {\n",
    "            'table_name': 'TABLE_NAME',\n",
    "            'rows_ingested': 'ROWS_INGESTED', \n",
    "            'batch_number': 'BATCH_NUMBER',\n",
    "            'batch_start_time': 'BATCH_START_TIME',\n",
    "            'batch_end_time': 'BATCH_END_TIME',\n",
    "            'run_id': 'RUN_ID',\n",
    "            'service_type': 'SERVICE_TYPE',\n",
    "            'source_year': 'SOURCE_YEAR',\n",
    "            'source_month': 'SOURCE_MONTH',\n",
    "            'ingested_at_utc': 'INGESTED_AT_UTC',\n",
    "            'source_path': 'SOURCE_PATH',\n",
    "            'file_size_bytes': 'FILE_SIZE_BYTES',\n",
    "            'original_file_rows': 'ORIGINAL_FILE_ROWS',\n",
    "            'natural_key_strategy': 'NATURAL_KEY_STRATEGY'\n",
    "        }\n",
    "        \n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in audit_df.columns:\n",
    "                audit_df = audit_df.withColumnRenamed(old_col, new_col)\n",
    "        \n",
    "        # Agregar timestamp de auditor√≠a\n",
    "        audit_df = audit_df.withColumn('AUDIT_TIMESTAMP_UTC', F.lit(datetime.utcnow()))\n",
    "        \n",
    "        # Cargar auditor√≠a\n",
    "        audit_df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", \"RAW_AUDITORIA\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"‚úÖ Auditor√≠a registrada con {len(auditoria)} entradas de lotes\")\n",
    "        \n",
    "        # Mostrar resumen de auditor√≠a\n",
    "        print(f\"\\nüìä RESUMEN DE AUDITOR√çA:\")\n",
    "        audit_summary = audit_df.groupBy('TABLE_NAME', 'SERVICE_TYPE', 'SOURCE_YEAR', 'SOURCE_MONTH') \\\n",
    "            .agg(\n",
    "                F.sum('ROWS_INGESTED').alias('TOTAL_ROWS'),\n",
    "                F.max('BATCH_NUMBER').alias('MAX_BATCH'),\n",
    "                F.min('BATCH_START_TIME').alias('START_TIME'),\n",
    "                F.max('BATCH_END_TIME').alias('END_TIME')\n",
    "            ).collect()\n",
    "        \n",
    "        for row in audit_summary:\n",
    "            print(f\"   {row['TABLE_NAME']}: {row['TOTAL_ROWS']:,} filas en {row['MAX_BATCH']} lotes\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error registrando auditor√≠a: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüéâ INGESTA COMPLETADA - RESUMEN FINAL\")\n",
    "print(f\"‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\")\n",
    "print(f\"üìã ESTRATEGIA IMPLEMENTADA:\")\n",
    "print(f\"   ‚Ä¢ Partici√≥n l√≥gica por a√±o/mes en tablas por servicio\")\n",
    "print(f\"   ‚Ä¢ Idempotencia con DELETE + INSERT por partici√≥n\")\n",
    "print(f\"   ‚Ä¢ Clave natural robusta: pickup_datetime|pu|do|vendor|distance\")\n",
    "print(f\"   ‚Ä¢ Metadatos completos por registro y lote\")\n",
    "print(f\"\")\n",
    "print(f\"üìä ESTAD√çSTICAS:\")\n",
    "print(f\"   ‚Ä¢ Run ID: {run_id}\")\n",
    "print(f\"   ‚Ä¢ Archivos procesados: {total_files_processed}\")\n",
    "print(f\"   ‚Ä¢ Total filas ingested: {total_rows_ingested:,}\")\n",
    "print(f\"   ‚Ä¢ Timestamp conversi√≥n mejorada: ‚úÖ\")\n",
    "print(f\"   ‚Ä¢ Auditor√≠a completa: ‚úÖ\")\n",
    "\n",
    "# NO cerrar Spark aqu√≠ para evitar problemas de conexi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b65ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONFIGURACI√ìN B√ÅSICA ===\n",
      "Run ID: run_20251019_010247\n",
      "Timestamp: 2025-10-19T01:02:47.729833\n",
      "‚úÖ Configuraci√≥n lista\n"
     ]
    }
   ],
   "source": [
    "# INGESTA SIMPLE A SNOWFLAKE RAW - VERSI√ìN FUNCIONAL\n",
    "# \n",
    "# ESTRATEGIA ELEGIDA: \n",
    "# - Una tabla por servicio (GREEN_TAXI, YELLOW_TAXI) con partici√≥n l√≥gica por a√±o/mes\n",
    "# - Metadatos obligatorios en cada registro\n",
    "# - Clave natural para idempotencia: pickup_datetime + PULocationID + DOLocationID + VendorID\n",
    "# - Sin upserts, solo INSERT directo a capa RAW\n",
    "#\n",
    "# JUSTIFICACI√ìN:\n",
    "# - Mantiene esquemas espec√≠ficos por servicio\n",
    "# - Permite consultas eficientes con filtros por a√±o/mes\n",
    "# - Auditor√≠a completa con metadatos en cada registro\n",
    "\n",
    "import os\n",
    "import pyspark \n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# 1. Configuraci√≥n b√°sica\n",
    "print(\"=== CONFIGURACI√ìN B√ÅSICA ===\")\n",
    "\n",
    "# Variables de configuraci√≥n\n",
    "service_types = ['green']  # Empezamos solo con green\n",
    "start_year = 2015\n",
    "end_year = 2015\n",
    "months = [1, 2]  # Solo enero y febrero para prueba\n",
    "data_dir = '/home/jovyan/work/data'\n",
    "\n",
    "# Generar run_id √∫nico\n",
    "run_id = f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "ingested_at_utc = datetime.utcnow().isoformat()\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Timestamp: {ingested_at_utc}\")\n",
    "\n",
    "# 2. Configuraci√≥n Snowflake\n",
    "sfOptions = {\n",
    "    \"sfURL\": \"YKFGMFI-GRC01155.snowflakecomputing.com\",\n",
    "    \"sfUser\": \"MARTIN\",\n",
    "    \"sfPassword\": \"P7kh2nUSu727FKZ\",\n",
    "    \"sfDatabase\": \"NY_TAXI\",\n",
    "    \"sfSchema\": \"RAW\",\n",
    "    \"sfWarehouse\": \"COMPUTE_WH\",\n",
    "    \"sfRole\": \"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n lista\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9621da75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones auxiliares definidas\n"
     ]
    }
   ],
   "source": [
    "# 3. Funci√≥n para convertir timestamps de epoch milisegundos a timestamp\n",
    "def convert_epoch_to_timestamp(df, column_name):\n",
    "    \"\"\"\n",
    "    Convierte columnas de epoch en milisegundos a timestamp\n",
    "    Ejemplo: 1420072270000 -> 2015-01-01 01:57:50\n",
    "    Verifica el tipo de datos antes de convertir\n",
    "    \"\"\"\n",
    "    if column_name in df.columns:\n",
    "        # Verificar el tipo de datos de la columna\n",
    "        current_type = df.schema[column_name].dataType\n",
    "        print(f\"üîç Tipo de datos de {column_name}: {current_type}\")\n",
    "        \n",
    "        # Si ya es timestamp, no hacer nada\n",
    "        if isinstance(current_type, T.TimestampType):\n",
    "            print(f\"‚úÖ {column_name} ya es timestamp, no necesita conversi√≥n\")\n",
    "            return df\n",
    "        \n",
    "        # Si es LongType (epoch), convertir\n",
    "        elif isinstance(current_type, T.LongType):\n",
    "            print(f\"üîÑ Convirtiendo {column_name} de epoch a timestamp\")\n",
    "            df = df.withColumn(\n",
    "                column_name, \n",
    "                F.when(F.col(column_name).isNull(), None)\n",
    "                 .when(F.col(column_name) == 0, None)\n",
    "                 .otherwise(F.from_unixtime(F.col(column_name) / 1000).cast(T.TimestampType()))\n",
    "            )\n",
    "            print(f\"‚úÖ Convertido {column_name} de epoch a timestamp\")\n",
    "        \n",
    "        # Si es StringType, intentar conversi√≥n directa\n",
    "        elif isinstance(current_type, T.StringType):\n",
    "            print(f\"üîÑ Convirtiendo {column_name} de string a timestamp\")\n",
    "            df = df.withColumn(\n",
    "                column_name,\n",
    "                F.when(F.col(column_name).isNull(), None)\n",
    "                 .when(F.col(column_name) == \"\", None)\n",
    "                 .otherwise(F.to_timestamp(F.col(column_name)))\n",
    "            )\n",
    "            print(f\"‚úÖ Convertido {column_name} de string a timestamp\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Tipo no soportado para {column_name}: {current_type}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Columna {column_name} no encontrada\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 4. Funci√≥n para agregar metadatos obligatorios\n",
    "def add_metadata(df, service_type, source_year, source_month, source_path):\n",
    "    \"\"\"\n",
    "    Agrega metadatos obligatorios a cada registro\n",
    "    \"\"\"\n",
    "    df = df.withColumn('run_id', F.lit(run_id)) \\\n",
    "           .withColumn('service_type', F.lit(service_type)) \\\n",
    "           .withColumn('source_year', F.lit(source_year)) \\\n",
    "           .withColumn('source_month', F.lit(source_month)) \\\n",
    "           .withColumn('ingested_at_utc', F.lit(ingested_at_utc)) \\\n",
    "           .withColumn('source_path', F.lit(source_path))\n",
    "    \n",
    "    print(f\"‚úÖ Metadatos agregados para {service_type} {source_year}-{source_month:02d}\")\n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c3e9f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Funciones de carga definidas\n"
     ]
    }
   ],
   "source": [
    "# 5. Funci√≥n para crear clave natural\n",
    "def create_natural_key(df, service_type):\n",
    "    \"\"\"\n",
    "    Crea clave natural para idempotencia\n",
    "    Formato: pickup_datetime|PULocationID|DOLocationID|VendorID\n",
    "    \"\"\"\n",
    "    # Determinar columna de pickup seg√∫n el servicio\n",
    "    pickup_col = 'lpep_pickup_datetime' if service_type == 'green' else 'tpep_pickup_datetime'\n",
    "    \n",
    "    # Crear componentes de la clave natural\n",
    "    components = [\n",
    "        F.coalesce(F.col(pickup_col).cast('string'), F.lit('NULL')),\n",
    "        F.coalesce(F.col('PULocationID').cast('string'), F.lit('NULL')),\n",
    "        F.coalesce(F.col('DOLocationID').cast('string'), F.lit('NULL')),\n",
    "        F.coalesce(F.col('VendorID').cast('string'), F.lit('NULL'))\n",
    "    ]\n",
    "    \n",
    "    # Concatenar con separador\n",
    "    df = df.withColumn('natural_key', F.concat_ws('|', *components))\n",
    "    \n",
    "    print(f\"‚úÖ Clave natural creada para {service_type}\")\n",
    "    return df\n",
    "\n",
    "# 6. Funci√≥n simple de carga a Snowflake\n",
    "def load_to_snowflake(df, table_name):\n",
    "    \"\"\"\n",
    "    Carga simple a Snowflake usando mode append\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üöÄ Cargando {df.count():,} registros a {table_name}\")\n",
    "        \n",
    "        df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(f\"‚úÖ Carga exitosa a {table_name}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando a {table_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úÖ Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9066c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIALIZANDO SPARK ===\n",
      "üöÄ Creando sesi√≥n Spark...\n",
      "‚úÖ Spark funcionando correctamente - Test: 5 registros\n",
      "‚úÖ Versi√≥n Spark: 3.5.0\n",
      "‚úÖ JARs configurados: 2 archivos\n"
     ]
    }
   ],
   "source": [
    "# INICIALIZAR SPARK SESSION\n",
    "print(\"=== INICIALIZANDO SPARK ===\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Configuraci√≥n de JARs para Snowflake\n",
    "jars_dir = '/home/jovyan/work/jars'\n",
    "spark_jars = f\"{jars_dir}/spark-snowflake_2.12-2.12.0-spark_3.4.jar,{jars_dir}/snowflake-jdbc-3.14.4.jar\"\n",
    "\n",
    "# Crear sesi√≥n Spark simple\n",
    "print(\"üöÄ Creando sesi√≥n Spark...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"NYC_Taxi_Ingesta_Simple\") \\\n",
    "    .config(\"spark.jars\", spark_jars) \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verificar que funciona\n",
    "test_count = spark.range(5).count()\n",
    "print(f\"‚úÖ Spark funcionando correctamente - Test: {test_count} registros\")\n",
    "print(f\"‚úÖ Versi√≥n Spark: {spark.version}\")\n",
    "\n",
    "# Mostrar configuraci√≥n activa\n",
    "print(f\"‚úÖ JARs configurados: {len(spark_jars.split(','))} archivos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INICIANDO PROCESAMIENTO ===\n",
      "\n",
      "üìã Procesando servicio: GREEN\n",
      "\n",
      "üìÅ Procesando: green_tripdata_2015-01.parquet\n",
      "   üìñ Leyendo archivo Parquet...\n",
      "   üìä Registros originales: 1,508,493\n",
      "   üïê Convirtiendo timestamps...\n",
      "üîç Tipo de datos de lpep_pickup_datetime: TimestampNTZType()\n",
      "‚ö†Ô∏è Tipo no soportado para lpep_pickup_datetime: TimestampNTZType()\n",
      "üîç Tipo de datos de lpep_dropoff_datetime: TimestampNTZType()\n",
      "‚ö†Ô∏è Tipo no soportado para lpep_dropoff_datetime: TimestampNTZType()\n",
      "   üìã Agregando metadatos...\n",
      "‚úÖ Metadatos agregados para green 2015-01\n",
      "   üîë Creando clave natural...\n",
      "‚úÖ Clave natural creada para green\n",
      "   üßπ Eliminando duplicados...\n",
      "   üìä Duplicados eliminados: 3,278\n"
     ]
    }
   ],
   "source": [
    "# 7. PROCESAMIENTO PRINCIPAL - PASO A PASO\n",
    "print(\"=== INICIANDO PROCESAMIENTO ===\")\n",
    "\n",
    "# Contadores para auditor√≠a\n",
    "total_files_processed = 0\n",
    "total_rows_ingested = 0\n",
    "audit_records = []\n",
    "\n",
    "for service_type in service_types:\n",
    "    print(f\"\\nüìã Procesando servicio: {service_type.upper()}\")\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in months:\n",
    "            # Construir nombre de archivo\n",
    "            fname = f'{service_type}_tripdata_{year}-{month:02d}.parquet'\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            \n",
    "            print(f\"\\nüìÅ Procesando: {fname}\")\n",
    "            \n",
    "            # Verificar si existe el archivo\n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"‚ö†Ô∏è Archivo no encontrado: {fname}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # PASO 1: Leer archivo Parquet\n",
    "                print(\"   üìñ Leyendo archivo Parquet...\")\n",
    "                df = spark.read.parquet(fpath)\n",
    "                original_count = df.count()\n",
    "                print(f\"   üìä Registros originales: {original_count:,}\")\n",
    "                \n",
    "                if original_count == 0:\n",
    "                    print(\"   ‚ö†Ô∏è Archivo vac√≠o, saltando...\")\n",
    "                    continue\n",
    "                \n",
    "                # PASO 2: Convertir timestamps de epoch a timestamp\n",
    "                print(\"   üïê Convirtiendo timestamps...\")\n",
    "                if service_type == 'green':\n",
    "                    df = convert_epoch_to_timestamp(df, 'lpep_pickup_datetime')\n",
    "                    df = convert_epoch_to_timestamp(df, 'lpep_dropoff_datetime')\n",
    "                else:  # yellow\n",
    "                    df = convert_epoch_to_timestamp(df, 'tpep_pickup_datetime')\n",
    "                    df = convert_epoch_to_timestamp(df, 'tpep_dropoff_datetime')\n",
    "                \n",
    "                # PASO 3: Agregar metadatos obligatorios\n",
    "                print(\"   üìã Agregando metadatos...\")\n",
    "                df = add_metadata(df, service_type, year, month, fpath)\n",
    "                \n",
    "                # PASO 4: Crear clave natural\n",
    "                print(\"   üîë Creando clave natural...\")\n",
    "                df = create_natural_key(df, service_type)\n",
    "                \n",
    "                # PASO 5: Eliminar duplicados usando clave natural\n",
    "                print(\"   üßπ Eliminando duplicados...\")\n",
    "                count_before = df.count()\n",
    "                df = df.dropDuplicates(['natural_key'])\n",
    "                count_after = df.count()\n",
    "                duplicates_removed = count_before - count_after\n",
    "                print(f\"   üìä Duplicados eliminados: {duplicates_removed:,}\")\n",
    "                \n",
    "                # PASO 6: Cargar a Snowflake\n",
    "                table_name = \"GREEN_TAXI\" if service_type == 'green' else \"YELLOW_TAXI\"\n",
    "                success = load_to_snowflake(df, table_name)\n",
    "                \n",
    "                if success:\n",
    "                    total_files_processed += 1\n",
    "                    total_rows_ingested += count_after\n",
    "                    \n",
    "                    # Registrar para auditor√≠a\n",
    "                    audit_records.append({\n",
    "                        'table': table_name,\n",
    "                        'rows_ingested': count_after,\n",
    "                        'run_id': run_id,\n",
    "                        'service_type': service_type,\n",
    "                        'source_year': year,\n",
    "                        'source_month': month,\n",
    "                        'ingested_at_utc': ingested_at_utc,\n",
    "                        'source_path': fpath\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"   ‚úÖ {count_after:,} registros cargados exitosamente\")\n",
    "                else:\n",
    "                    print(f\"   ‚ùå Error en la carga\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error procesando {fname}: {e}\")\n",
    "                continue\n",
    "\n",
    "print(f\"\\nüéâ PROCESAMIENTO COMPLETADO\")\n",
    "print(f\"üìä Archivos procesados: {total_files_processed}\")\n",
    "print(f\"üìä Total registros ingested: {total_rows_ingested:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83420b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== REGISTRANDO AUDITOR√çA ===\n",
      "‚ö†Ô∏è No hay registros para auditor√≠a\n",
      "\n",
      "üèÅ PROCESO COMPLETADO\n",
      "Run ID: run_20251019_003137\n",
      "Archivos procesados: 0\n",
      "Total registros: 0\n"
     ]
    }
   ],
   "source": [
    "# 8. REGISTRAR AUDITOR√çA\n",
    "print(\"\\n=== REGISTRANDO AUDITOR√çA ===\")\n",
    "\n",
    "if audit_records:\n",
    "    try:\n",
    "        # Crear DataFrame de auditor√≠a\n",
    "        print(f\"üìù Creando registro de auditor√≠a con {len(audit_records)} entradas...\")\n",
    "        \n",
    "        audit_df = spark.createDataFrame(audit_records)\n",
    "        \n",
    "        # Cargar a tabla de auditor√≠a\n",
    "        audit_df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", \"RAW_AUDITORIA\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(\"‚úÖ Auditor√≠a registrada exitosamente\")\n",
    "        \n",
    "        # Mostrar resumen de auditor√≠a\n",
    "        print(\"\\nüìä RESUMEN DE AUDITOR√çA:\")\n",
    "        for record in audit_records:\n",
    "            print(f\"   {record['table']}: {record['rows_ingested']:,} registros ({record['source_year']}-{record['source_month']:02d})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error registrando auditor√≠a: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No hay registros para auditor√≠a\")\n",
    "\n",
    "print(f\"\\nüèÅ PROCESO COMPLETADO\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"Archivos procesados: {total_files_processed}\")\n",
    "print(f\"Total registros: {total_rows_ingested:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb6e4ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INGESTA RAW PURA (COMPATIBLE SNOWFLAKE) ===\n",
      "Raw Run ID: raw_20251019_005946\n",
      "‚úÖ Funciones RAW compatibles definidas\n"
     ]
    }
   ],
   "source": [
    "# INGESTA RAW PURA - SIN TRANSFORMACIONES (CON COMPATIBILIDAD SNOWFLAKE)\n",
    "# \n",
    "# PRINCIPIO: Datos tal como vienen + metadatos m√≠nimos obligatorios\n",
    "# - Preservar valores originales (no transformar contenido)\n",
    "# - Solo ajustar tipos para compatibilidad con Snowflake \n",
    "# - Agregar metadatos de ingesta obligatorios\n",
    "# - Eliminar duplicados ANTES de cargar (para idempotencia)\n",
    "\n",
    "print(\"=== INGESTA RAW PURA (COMPATIBLE SNOWFLAKE) ===\")\n",
    "\n",
    "# 1. Configuraci√≥n m√≠nima\n",
    "service_types = ['green']\n",
    "start_year = 2015\n",
    "end_year = 2015  \n",
    "months = [1]\n",
    "data_dir = '/home/jovyan/work/data'\n",
    "\n",
    "# Metadatos de run\n",
    "run_id = f\"raw_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "ingested_at_utc = datetime.utcnow().isoformat()\n",
    "\n",
    "print(f\"Raw Run ID: {run_id}\")\n",
    "\n",
    "# 2. Funci√≥n para hacer tipos compatibles con Snowflake (SIN cambiar valores)\n",
    "def make_snowflake_compatible(df):\n",
    "    \"\"\"\n",
    "    Convierte solo tipos problem√°ticos para Snowflake\n",
    "    PRESERVA los valores originales - solo cambia el tipo de dato\n",
    "    \"\"\"\n",
    "    print(\"   üîß Ajustando tipos para Snowflake (preservando valores)...\")\n",
    "    \n",
    "    for field in df.schema.fields:\n",
    "        col_name = field.name\n",
    "        current_type = field.dataType\n",
    "        \n",
    "        # Convertir TimestampNTZType a String para preservar valores exactos\n",
    "        if isinstance(current_type, T.TimestampType):\n",
    "            print(f\"     üìÖ {col_name}: TimestampType ‚Üí String (preservando valores)\")\n",
    "            df = df.withColumn(col_name, F.col(col_name).cast(T.StringType()))\n",
    "        \n",
    "        # Otros tipos problem√°ticos se pueden agregar aqu√≠ si aparecen\n",
    "        \n",
    "    return df\n",
    "\n",
    "# 3. Funci√≥n RAW - sin transformaciones de contenido\n",
    "def add_raw_metadata_only(df, service_type, source_year, source_month, source_path):\n",
    "    \"\"\"\n",
    "    Solo agrega metadatos obligatorios - NO transforma datos originales\n",
    "    \"\"\"\n",
    "    return df.withColumn('raw_run_id', F.lit(run_id)) \\\n",
    "             .withColumn('raw_service_type', F.lit(service_type)) \\\n",
    "             .withColumn('raw_source_year', F.lit(source_year)) \\\n",
    "             .withColumn('raw_source_month', F.lit(source_month)) \\\n",
    "             .withColumn('raw_ingested_at_utc', F.lit(ingested_at_utc)) \\\n",
    "             .withColumn('raw_source_path', F.lit(source_path))\n",
    "\n",
    "# 4. Clave natural RAW - usando datos originales\n",
    "def create_raw_natural_key(df, service_type):\n",
    "    \"\"\"\n",
    "    Crea clave natural con datos originales (sin transformar)\n",
    "    \"\"\"\n",
    "    pickup_col = 'lpep_pickup_datetime' if service_type == 'green' else 'tpep_pickup_datetime'\n",
    "    \n",
    "    components = [\n",
    "        F.coalesce(F.col(pickup_col).cast('string'), F.lit('NULL')),\n",
    "        F.coalesce(F.col('PULocationID').cast('string'), F.lit('NULL')),  \n",
    "        F.coalesce(F.col('DOLocationID').cast('string'), F.lit('NULL')),\n",
    "        F.coalesce(F.col('VendorID').cast('string'), F.lit('NULL'))\n",
    "    ]\n",
    "    \n",
    "    return df.withColumn('raw_natural_key', F.concat_ws('|', *components))\n",
    "\n",
    "print(\"‚úÖ Funciones RAW compatibles definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1729a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5. Funci√≥n para convertir √©poca Unix a timestamp legible (solo si es necesario)\n",
    "def convert_epoch_columns_to_timestamp(df):\n",
    "    \"\"\"\n",
    "    Convierte columnas con tiempo en formato Unix epoch (milisegundos) a timestamp legible\n",
    "    SOLO si la columna es num√©rica (Long/Integer). Si ya es timestamp, la deja tal como est√°.\n",
    "    \"\"\"\n",
    "    print(\"   üïê Verificando y convirtiendo columnas de √©poca Unix...\")\n",
    "    \n",
    "    # Columnas que potencialmente contienen √©poca Unix en milisegundos \n",
    "    potential_epoch_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'tpep_pickup_datetime', 'tpep_dropoff_datetime']\n",
    "    \n",
    "    for col_name in potential_epoch_columns:\n",
    "        if col_name in df.columns:\n",
    "            # Obtener el tipo de dato de la columna\n",
    "            col_type = None\n",
    "            for field in df.schema.fields:\n",
    "                if field.name == col_name:\n",
    "                    col_type = field.dataType\n",
    "                    break\n",
    "            \n",
    "            # Solo convertir si es un tipo num√©rico (LongType, IntegerType)\n",
    "            if isinstance(col_type, (T.LongType, T.IntegerType)):\n",
    "                print(f\"     üìÖ {col_name} es {type(col_type).__name__} - Convirtiendo de √©poca Unix (ms) a timestamp\")\n",
    "                df = df.withColumn(\n",
    "                    col_name, \n",
    "                    F.from_unixtime(F.col(col_name) / 1000).cast(T.TimestampType())\n",
    "                )\n",
    "            elif isinstance(col_type, T.TimestampType):\n",
    "                print(f\"     ‚úÖ {col_name} ya es TimestampType - No requiere conversi√≥n\")\n",
    "            else:\n",
    "                print(f\"     ‚ö†Ô∏è {col_name} es {type(col_type).__name__} - Tipo no reconocido para fecha\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ef5348f9-a8aa-4b20-84e2-61244a38b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      "\n",
      "+---------------------+\n",
      "|lpep_dropoff_datetime|\n",
      "+---------------------+\n",
      "|  2015-01-01 00:50:41|\n",
      "|  2015-01-01 00:03:30|\n",
      "|  2015-01-01 00:33:26|\n",
      "|  2015-01-01 00:27:07|\n",
      "|  2015-01-01 00:40:32|\n",
      "|  2015-01-01 01:01:00|\n",
      "|  2015-01-01 01:15:52|\n",
      "|  2015-01-01 00:10:52|\n",
      "|  2015-01-01 00:18:55|\n",
      "|  2015-01-01 00:46:26|\n",
      "|  2015-01-01 01:10:09|\n",
      "|  2015-01-01 01:20:19|\n",
      "|  2015-01-01 00:39:42|\n",
      "|  2015-01-01 01:14:53|\n",
      "|  2015-01-01 00:47:57|\n",
      "|  2015-01-01 01:29:36|\n",
      "|  2015-01-01 00:19:41|\n",
      "|  2015-01-01 00:28:00|\n",
      "|  2015-01-01 00:49:47|\n",
      "|  2015-01-01 00:55:29|\n",
      "+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/work/data/green_tripdata_2015-01.parquet\")\n",
    "df.printSchema()\n",
    "df.select(\"lpep_dropoff_datetime\").show()\n",
    "df = df.withColumn(\n",
    "    \"lpep_dropoff_datetime\",\n",
    "    to_timestamp(col(\"lpep_dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52894866-94c7-4fe3-8c59-6db6a9528a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a3de52b-8c16-4504-ab81-a6c58fed8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|lpep_dropoff_datetime|\n",
      "+---------------------+\n",
      "|  2015-01-01 00:50:41|\n",
      "|  2015-01-01 00:03:30|\n",
      "|  2015-01-01 00:33:26|\n",
      "|  2015-01-01 00:27:07|\n",
      "|  2015-01-01 00:40:32|\n",
      "|  2015-01-01 01:01:00|\n",
      "|  2015-01-01 01:15:52|\n",
      "|  2015-01-01 00:10:52|\n",
      "|  2015-01-01 00:18:55|\n",
      "|  2015-01-01 00:46:26|\n",
      "|  2015-01-01 01:10:09|\n",
      "|  2015-01-01 01:20:19|\n",
      "|  2015-01-01 00:39:42|\n",
      "|  2015-01-01 01:14:53|\n",
      "|  2015-01-01 00:47:57|\n",
      "|  2015-01-01 01:29:36|\n",
      "|  2015-01-01 00:19:41|\n",
      "|  2015-01-01 00:28:00|\n",
      "|  2015-01-01 00:49:47|\n",
      "|  2015-01-01 00:55:29|\n",
      "+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"lpep_dropoff_datetime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613efc7-3d9f-47a9-8b4d-620775d9afd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ecd0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6. Funci√≥n de diagn√≥stico para mostrar esquema\n",
    "def show_dataframe_schema(df, label=\"DataFrame\"):\n",
    "    \"\"\"\n",
    "    Muestra el esquema del DataFrame para diagn√≥stico\n",
    "    \"\"\"\n",
    "    print(f\"   üìã Esquema de {label}:\")\n",
    "    for field in df.schema.fields:\n",
    "        print(f\"     - {field.name}: {type(field.dataType).__name__}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec2a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PROCESAMIENTO RAW (SIN TRANSFORMACIONES) ===\n",
      "\n",
      "üìã Servicio: GREEN\n",
      "\n",
      "üìÅ Procesando: green_tripdata_2015-01.parquet\n",
      "   üìñ Leyendo archivo RAW...\n",
      "   üìä Registros: 1,508,493\n",
      "   üïê Convirtiendo √©poca Unix a timestamp...\n",
      "   üïê Convirtiendo columnas de √©poca Unix a timestamp...\n",
      "     üìÖ Convirtiendo lpep_pickup_datetime de √©poca Unix (ms) a timestamp\n",
      "   ‚ùå Error: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(lpep_pickup_datetime / 1000)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"TIMESTAMP_NTZ\" and \"INT\").;\n",
      "'Project [VendorID#3664L, cast(from_unixtime((lpep_pickup_datetime#3665 / 1000), yyyy-MM-dd HH:mm:ss, Some(Etc/UTC)) as timestamp) AS lpep_pickup_datetime#3729, lpep_dropoff_datetime#3666, store_and_fwd_flag#3667, RatecodeID#3668L, PULocationID#3669L, DOLocationID#3670L, passenger_count#3671L, trip_distance#3672, fare_amount#3673, extra#3674, mta_tax#3675, tip_amount#3676, tolls_amount#3677, ehail_fee#3678, improvement_surcharge#3679, total_amount#3680, payment_type#3681L, trip_type#3682, congestion_surcharge#3683]\n",
      "+- Relation [VendorID#3664L,lpep_pickup_datetime#3665,lpep_dropoff_datetime#3666,store_and_fwd_flag#3667,RatecodeID#3668L,PULocationID#3669L,DOLocationID#3670L,passenger_count#3671L,trip_distance#3672,fare_amount#3673,extra#3674,mta_tax#3675,tip_amount#3676,tolls_amount#3677,ehail_fee#3678,improvement_surcharge#3679,total_amount#3680,payment_type#3681L,trip_type#3682,congestion_surcharge#3683] parquet\n",
      "\n",
      "\n",
      "üéâ INGESTA RAW COMPLETADA\n",
      "üìä Archivos: 0\n",
      "üìä Registros: 0\n",
      "üìä Estrategia: DATOS ORIGINALES + METADATOS M√çNIMOS\n"
     ]
    }
   ],
   "source": [
    "# 4. PROCESAMIENTO RAW PURO\n",
    "print(\"\\n=== PROCESAMIENTO RAW (SIN TRANSFORMACIONES) ===\")\n",
    "\n",
    "total_files_processed = 0\n",
    "total_rows_ingested = 0  \n",
    "audit_records = []\n",
    "\n",
    "for service_type in service_types:\n",
    "    print(f\"\\nüìã Servicio: {service_type.upper()}\")\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in months:\n",
    "            fname = f'{service_type}_tripdata_{year}-{month:02d}.parquet'\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            \n",
    "            print(f\"\\nüìÅ Procesando: {fname}\")\n",
    "            \n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"‚ö†Ô∏è Archivo no encontrado\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # PASO 1: Leer tal como viene\n",
    "                print(\"   üìñ Leyendo archivo RAW...\")\n",
    "                df = spark.read.parquet(fpath)\n",
    "                original_count = df.count()\n",
    "                print(f\"   üìä Registros: {original_count:,}\")\n",
    "                \n",
    "                # PASO 1.1: Mostrar esquema original para diagn√≥stico\n",
    "                df = show_dataframe_schema(df, \"archivo original\")\n",
    "                \n",
    "                if original_count == 0:\n",
    "                    continue\n",
    "                \n",
    "                # PASO 1.5: Convertir √©poca Unix a timestamp legible\n",
    "                print(\"   üïê Convirtiendo √©poca Unix a timestamp...\")\n",
    "                df = convert_epoch_columns_to_timestamp(df)\n",
    "                \n",
    "                # PASO 2: Hacer tipos compatibles con Snowflake\n",
    "                print(\"   üîß Ajustando tipos para Snowflake...\")\n",
    "                df = make_snowflake_compatible(df)\n",
    "                \n",
    "                # PASO 3: Solo agregar metadatos RAW (sin transformar datos)\n",
    "                print(\"   üìã Agregando metadatos RAW...\")\n",
    "                df = add_raw_metadata_only(df, service_type, year, month, fpath)\n",
    "                \n",
    "                # PASO 4: Clave natural con datos originales\n",
    "                print(\"   üîë Clave natural RAW...\")\n",
    "                df = create_raw_natural_key(df, service_type)\n",
    "                \n",
    "                # PASO 5: Eliminar duplicados (√∫nica \"transformaci√≥n\" permitida)\n",
    "                print(\"   üßπ Eliminando duplicados...\")\n",
    "                count_before = df.count()\n",
    "                df = df.dropDuplicates(['raw_natural_key'])\n",
    "                count_after = df.count()\n",
    "                print(f\"   üìä {count_before - count_after:,} duplicados eliminados\")\n",
    "                \n",
    "                # PASO 6: Mostrar esquema original preservado\n",
    "                print(\"   üìã Esquema preservado:\")\n",
    "                original_columns = [col for col in df.columns if not col.startswith('raw_')]\n",
    "                print(f\"   üìä Columnas originales: {len(original_columns)}\")\n",
    "                print(f\"   üìä Metadatos agregados: {len([col for col in df.columns if col.startswith('raw_')])}\")\n",
    "                \n",
    "                # PASO 7: Cargar a Snowflake RAW\n",
    "                table_name = \"GREEN_TAXI\" if service_type == 'green' else \"YELLOW_TAXI\"\n",
    "                print(f\"   üöÄ Cargando {count_after:,} registros RAW a {table_name}\")\n",
    "                \n",
    "                df.write \\\n",
    "                    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "                    .options(**sfOptions) \\\n",
    "                    .option(\"dbtable\", table_name) \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save()\n",
    "                \n",
    "                print(f\"   ‚úÖ Carga RAW exitosa\")\n",
    "                \n",
    "                # Auditor√≠a\n",
    "                audit_records.append({\n",
    "                    'table': table_name,\n",
    "                    'rows_ingested': count_after,\n",
    "                    'run_id': run_id,\n",
    "                    'service_type': service_type,\n",
    "                    'source_year': year,\n",
    "                    'source_month': month,\n",
    "                    'ingested_at_utc': ingested_at_utc,\n",
    "                    'source_path': fpath\n",
    "                })\n",
    "                \n",
    "                total_files_processed += 1\n",
    "                total_rows_ingested += count_after\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "                continue\n",
    "\n",
    "print(f\"\\nüéâ INGESTA RAW COMPLETADA\")\n",
    "print(f\"üìä Archivos: {total_files_processed}\")\n",
    "print(f\"üìä Registros: {total_rows_ingested:,}\")\n",
    "print(f\"üìä Estrategia: DATOS ORIGINALES + METADATOS M√çNIMOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0ddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. AUDITOR√çA RAW\n",
    "print(\"\\n=== REGISTRANDO AUDITOR√çA RAW ===\")\n",
    "\n",
    "if audit_records:\n",
    "    try:\n",
    "        audit_df = spark.createDataFrame(audit_records)\n",
    "        \n",
    "        audit_df.write \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"dbtable\", \"RAW_AUDITORIA\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "        \n",
    "        print(\"‚úÖ Auditor√≠a RAW registrada\")\n",
    "        \n",
    "        for record in audit_records:\n",
    "            print(f\"   {record['table']}: {record['rows_ingested']:,} registros\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error auditor√≠a: {e}\")\n",
    "\n",
    "print(f\"\\nüèÅ PROCESO RAW COMPLETADO\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "# 6. VERIFICACI√ìN RAW\n",
    "print(\"\\n=== VERIFICACI√ìN RAW ===\")\n",
    "\n",
    "try:\n",
    "    green_count = spark.read \\\n",
    "        .format(\"net.snowflake.spark.snowflake\") \\\n",
    "        .options(**sfOptions) \\\n",
    "        .option(\"query\", f\"SELECT COUNT(*) as count FROM GREEN_TAXI WHERE raw_run_id = '{run_id}'\") \\\n",
    "        .load() \\\n",
    "        .collect()[0]['count']\n",
    "    \n",
    "    print(f\"‚úÖ GREEN_TAXI RAW: {green_count:,} registros\")\n",
    "    \n",
    "    # Mostrar muestra de datos RAW (con timestamps originales)\n",
    "    if green_count > 0:\n",
    "        print(\"\\nüîç Muestra datos RAW (timestamps en formato original):\")\n",
    "        sample = spark.read \\\n",
    "            .format(\"net.snowflake.spark.snowflake\") \\\n",
    "            .options(**sfOptions) \\\n",
    "            .option(\"query\", f\"SELECT lpep_pickup_datetime, lpep_dropoff_datetime, raw_run_id FROM GREEN_TAXI WHERE raw_run_id = '{run_id}' LIMIT 3\") \\\n",
    "            .load()\n",
    "        \n",
    "        sample.show(3, truncate=False)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error verificaci√≥n: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ INGESTA RAW PURA COMPLETADA\")\n",
    "print(\"üìã PRINCIPIOS RESPETADOS:\")\n",
    "print(\"   ‚Ä¢ Datos originales preservados\")\n",
    "print(\"   ‚Ä¢ Solo metadatos de ingesta agregados\")  \n",
    "print(\"   ‚Ä¢ Sin transformaciones de tipos/valores\")\n",
    "print(\"   ‚Ä¢ Timestamps en formato original (epoch)\")\n",
    "print(\"   ‚Ä¢ Deduplicaci√≥n antes de carga (idempotencia)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "587bdec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INGESTA RAW CON CONVERSI√ìN FORZADA DE TIPOS ===\n",
      "Run ID: raw_fixed_20251019_020617\n",
      "\n",
      "üìã Servicio: GREEN\n",
      "\n",
      "üìÅ Procesando: green_tripdata_2015-01.parquet\n",
      "   üìñ Leyendo archivo...\n",
      "   üìä Registros: 1,508,493\n",
      "   üîß Convirtiendo SOLO pickup/dropoff a TIMESTAMP_NTZ(9)...\n",
      "     üìÖ lpep_pickup_datetime: TimestampNTZType() ‚Üí TIMESTAMP_NTZ(9) format\n",
      "     üìÖ lpep_dropoff_datetime: TimestampNTZType() ‚Üí TIMESTAMP_NTZ(9) format\n",
      "   ‚úÖ Otros tipos mantenidos sin cambios\n",
      " Agregando metadatos...\n",
      "   üßπ Eliminando duplicados...\n",
      "   üìä 303,246 duplicados eliminados\n",
      "   üöÄ Cargando 1,205,247 registros a GREEN_TAXI\n",
      "   ‚ùå Error: An error occurred while calling o903.save.\n",
      ": java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Numeric value '2015-01-01 00:16:25.000000000' is not recognized\n",
      "  File 'iHzq9zR9pa/1.CSV.gz', line 1, character 36\n",
      "  Row 1, column \"GREEN_TAXI\"[\"SOURCE_YEAR\":3]\n",
      "  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.\n",
      "\tat net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:140)\n",
      "\tat net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:279)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:567)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:447)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:287)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:232)\n",
      "\tat net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)\n",
      "\tat net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:74)\n",
      "\tat net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "\n",
      "üéâ PROCESO COMPLETADO\n",
      "üìä Archivos procesados: 0\n",
      "üìä Total registros: 0\n",
      "‚úÖ Timestamps convertidos a formato TIMESTAMP_NTZ(9) compatible\n",
      "\n",
      "üìã SIGUIENTE PASO EN SNOWFLAKE:\n",
      "   ALTER TABLE GREEN_TAXI ALTER COLUMN lpep_pickup_datetime SET DATA TYPE TIMESTAMP_NTZ(9);\n",
      "   ALTER TABLE GREEN_TAXI ALTER COLUMN lpep_dropoff_datetime SET DATA TYPE TIMESTAMP_NTZ(9);\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_219/205834128.py\", line 109, in <module>\n",
      "    .save()\n",
      "     ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/sql/readwriter.py\", line 1461, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o903.save.\n",
      ": java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Numeric value '2015-01-01 00:16:25.000000000' is not recognized\n",
      "  File 'iHzq9zR9pa/1.CSV.gz', line 1, character 36\n",
      "  Row 1, column \"GREEN_TAXI\"[\"SOURCE_YEAR\":3]\n",
      "  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.\n",
      "\tat net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:140)\n",
      "\tat net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:279)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:567)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:447)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:287)\n",
      "\tat net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:232)\n",
      "\tat net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)\n",
      "\tat net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:74)\n",
      "\tat net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SOLUCI√ìN DEFINITIVA AL ERROR SNOWFLAKE\n",
    "# Convertir TODOS los tipos problem√°ticos a tipos compatibles\n",
    "\n",
    "print(\"=== INGESTA RAW CON CONVERSI√ìN FORZADA DE TIPOS ===\")\n",
    "\n",
    "# Configuraci√≥n\n",
    "service_types = ['green']\n",
    "start_year = 2015\n",
    "end_year = 2015  \n",
    "months = [1]\n",
    "data_dir = '/home/jovyan/work/data'\n",
    "run_id = f\"raw_fixed_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "ingested_at_utc = datetime.utcnow().isoformat()\n",
    "\n",
    "print(f\"Run ID: {run_id}\")\n",
    "\n",
    "def convert_all_to_snowflake_types(df):\n",
    "    \"\"\"\n",
    "    SOLO convierte pickup_datetime y dropoff_datetime a TIMESTAMP_NTZ(9)\n",
    "    Mantiene todas las dem√°s columnas tal como est√°n\n",
    "    \"\"\"\n",
    "    print(\"   üîß Convirtiendo SOLO pickup/dropoff a TIMESTAMP_NTZ(9)...\")\n",
    "    \n",
    "    for field in df.schema.fields:\n",
    "        col_name = field.name\n",
    "        current_type = field.dataType\n",
    "        \n",
    "        # SOLO convertir timestamps de pickup/dropoff a formato TIMESTAMP_NTZ(9)\n",
    "        if (\"pickup_datetime\" in col_name.lower() or \"dropoff_datetime\" in col_name.lower()):\n",
    "            if \"timestamp\" in str(current_type).lower():\n",
    "                print(f\"     üìÖ {col_name}: {current_type} ‚Üí TIMESTAMP_NTZ(9) format\")\n",
    "                df = df.withColumn(col_name, F.date_format(F.col(col_name), \"yyyy-MM-dd HH:mm:ss.SSSSSSSSS\"))\n",
    "            elif isinstance(current_type, T.LongType):\n",
    "                print(f\"     üïê {col_name}: LongType ‚Üí TIMESTAMP_NTZ(9) format\")\n",
    "                df = df.withColumn(col_name, F.date_format(F.from_unixtime(F.col(col_name) / 1000), \"yyyy-MM-dd HH:mm:ss.SSSSSSSSS\"))\n",
    "        \n",
    "        # MANTENER todas las dem√°s columnas sin cambios (comentado para claridad)\n",
    "        # No convertir otros timestamps - mantener tipos originales\n",
    "    \n",
    "    print(\"   ‚úÖ Otros tipos mantenidos sin cambios\")\n",
    "    return df\n",
    "\n",
    "# Procesamiento simplificado\n",
    "total_files_processed = 0\n",
    "total_rows_ingested = 0\n",
    "audit_records = []\n",
    "\n",
    "for service_type in service_types:\n",
    "    print(f\"\\nüìã Servicio: {service_type.upper()}\")\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in months:\n",
    "            fname = f'{service_type}_tripdata_{year}-{month:02d}.parquet'\n",
    "            fpath = os.path.join(data_dir, fname)\n",
    "            \n",
    "            print(f\"\\nüìÅ Procesando: {fname}\")\n",
    "            \n",
    "            if not os.path.isfile(fpath):\n",
    "                print(f\"‚ö†Ô∏è Archivo no encontrado\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # PASO 1: Leer archivo\n",
    "                print(\"   üìñ Leyendo archivo...\")\n",
    "                df = spark.read.parquet(fpath)\n",
    "                original_count = df.count()\n",
    "                print(f\"   üìä Registros: {original_count:,}\")\n",
    "                \n",
    "                if original_count == 0:\n",
    "                    continue\n",
    "                \n",
    "                # PASO 3: Convertir tipos problem√°ticos\n",
    "                df = convert_all_to_snowflake_types(df)\n",
    "                \n",
    "                # PASO 4: Agregar metadatos\n",
    "                print(\" Agregando metadatos...\")\n",
    "                df = df.withColumn('run_id', F.lit(run_id)) \\\n",
    "                       .withColumn('service_type', F.lit(service_type)) \\\n",
    "                       .withColumn('source_year', F.lit(year)) \\\n",
    "                       .withColumn('source_month', F.lit(month)) \\\n",
    "                       .withColumn('ingested_at_utc', F.lit(ingested_at_utc)) \\\n",
    "                       .withColumn('source_path', F.lit(fpath))\n",
    "                \n",
    "                # PASO 5: Clave natural simple\n",
    "                pickup_col = 'lpep_pickup_datetime' if service_type == 'green' else 'tpep_pickup_datetime'\n",
    "                df = df.withColumn('raw_natural_key', \n",
    "                    F.concat_ws('|',\n",
    "                        F.coalesce(F.col(pickup_col).cast('string'), F.lit('NULL')),\n",
    "                        F.coalesce(F.col('VendorID').cast('string'), F.lit('NULL'))\n",
    "                    ))\n",
    "                \n",
    "                # PASO 6: Eliminar duplicados\n",
    "                print(\"   üßπ Eliminando duplicados...\")\n",
    "                count_before = df.count()\n",
    "                df = df.dropDuplicates(['raw_natural_key'])\n",
    "                count_after = df.count()\n",
    "                print(f\"   üìä {count_before - count_after:,} duplicados eliminados\")\n",
    "            \n",
    "                # PASO 8: Cargar a Snowflake con configuraci√≥n TIMESTAMP_NTZ\n",
    "                table_name = \"GREEN_TAXI\" if service_type == 'green' else \"YELLOW_TAXI\"\n",
    "                print(f\"   üöÄ Cargando {count_after:,} registros a {table_name}\")\n",
    "                \n",
    "                df.write \\\n",
    "                    .format(\"net.snowflake.spark.snowflake\") \\\n",
    "                    .options(**sfOptions) \\\n",
    "                    .option(\"dbtable\", table_name) \\\n",
    "                    .option(\"keep_column_case\", \"on\") \\\n",
    "                    .mode(\"append\") \\\n",
    "                    .save()\n",
    "                \n",
    "                print(f\"   ‚úÖ ¬°CARGA EXITOSA!\")\n",
    "                \n",
    "                total_files_processed += 1\n",
    "                total_rows_ingested += count_after\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "print(f\"\\nüéâ PROCESO COMPLETADO\")\n",
    "print(f\"üìä Archivos procesados: {total_files_processed}\")\n",
    "print(f\"üìä Total registros: {total_rows_ingested:,}\")\n",
    "print(f\"‚úÖ Timestamps convertidos a formato TIMESTAMP_NTZ(9) compatible\")\n",
    "print(f\"\")\n",
    "print(f\"üìã SIGUIENTE PASO EN SNOWFLAKE:\")\n",
    "print(f\"   ALTER TABLE GREEN_TAXI ALTER COLUMN lpep_pickup_datetime SET DATA TYPE TIMESTAMP_NTZ(9);\")\n",
    "print(f\"   ALTER TABLE GREEN_TAXI ALTER COLUMN lpep_dropoff_datetime SET DATA TYPE TIMESTAMP_NTZ(9);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "23f57877-931e-42da-b480-b04be72147c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      " |-- raw_run_id: string (nullable = false)\n",
      " |-- raw_service_type: string (nullable = false)\n",
      " |-- raw_source_year: integer (nullable = false)\n",
      " |-- raw_source_month: integer (nullable = false)\n",
      " |-- raw_ingested_at_utc: string (nullable = false)\n",
      " |-- raw_source_path: string (nullable = false)\n",
      " |-- raw_natural_key: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.select(\"lpep_dropoff_datetime\").show()\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809d736-433a-4ed8-9e61-22cc3af43178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
